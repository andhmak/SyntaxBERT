{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Learning SQL embeddings for cardinality and cost estimation"
      ],
      "metadata": {
        "id": "zmbdos6WHJZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a notebook containing the code for the \"Learning SQL embeddings for cardinality and cost estimation\" BSc thesis at the National and Kapodistrian University of Athens."
      ],
      "metadata": {
        "id": "Hnf13W_jIXx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation"
      ],
      "metadata": {
        "id": "4HXMyzXwRnIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and decompress our datasets."
      ],
      "metadata": {
        "id": "YW831EACIp6p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpu8t4NdKdFY"
      },
      "outputs": [],
      "source": [
        "%cd <the path to where you have the download_datasets.sh file>\n",
        "!bash download_datasets.sh\n",
        "!tar -xvzf joblight_train.tar.gz\n",
        "!tar -xvzf jobm.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format all the data in such a way that they are usable by the models."
      ],
      "metadata": {
        "id": "QMQIom6jRar6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVGdTrjr-gc2",
        "outputId": "8af579c9-5a81-4d5e-fdca-d7d0b7f94e88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/thesis/joblight_train/joblight-train-all\n",
            "/content/drive/MyDrive/thesis/jobm/all_jobm\n"
          ]
        }
      ],
      "source": [
        "# necessary imports\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "# move to the correct directory\n",
        "%cd ./joblight_train/joblight-train-all\n",
        "\n",
        "# save the table sizes of the IMDB database\n",
        "imdb_table_sizes = {\"cast_info\": 36366040, \"comp_cast_type\": 4, \"company_type\": 4, \"complete_cast\": 126655,\n",
        "                    \"info_type\": 113, \"kind_type\": 7, \"link_type\": 18, \"movie_companies\": 2618265, \"movie_info\": 15076160,\n",
        "                    \"movie_info_idx\": 1376832, \"movie_keyword\": 4513320, \"movie_link\": 28468, \"role_type\": 12, \"title\": 2514512}\n",
        "\n",
        "# extract the necessary data for JOB-light\n",
        "job_light = []    # queries\n",
        "table_sizes = []  # table sizes\n",
        "labels = []       # cardinalities\n",
        "costs = []        # costs\n",
        "\n",
        "def load_qrep(fn):\n",
        "    assert \".pkl\" in fn\n",
        "    with open(fn, \"rb\") as f:\n",
        "        query = pickle.load(f)\n",
        "\n",
        "    return query\n",
        "\n",
        "for i in range(43294):\n",
        "  qfn = str(i)+\".pkl\"\n",
        "  qrep = load_qrep(qfn)\n",
        "  new_string = qrep['sql'].replace(' AS ', ' ')\n",
        "  tables = re.search('FROM(.*)WHERE', new_string).group(1)\n",
        "  counter = 0\n",
        "  table_size = [0, 0, 0, 0]\n",
        "  for word in imdb_table_sizes.keys():\n",
        "    if word in tables:\n",
        "      table_size[counter] = imdb_table_sizes[word]\n",
        "      counter += 1\n",
        "  job_light.append(new_string)\n",
        "  table_sizes.append(table_size)\n",
        "  labels.append(float(sum([cardinality['actual'] for cardinality in (subplan['cardinality'] for subplan in qrep['subset_graph']['nodes'][:])])))\n",
        "  costs.append(float(sum([exec_time['actual'] for exec_time in (subplan['exec_time'] for subplan in qrep['subset_graph']['nodes'][:])])))\n",
        "\n",
        "# move to the correct directory\n",
        "%cd ../../jobm/all_jobm\n",
        "\n",
        "# extract the necessary data for JOB\n",
        "jobm = []         # queries\n",
        "table_sizesm = [] # table sizes\n",
        "labelsm = []      # cardinalities\n",
        "\n",
        "for i in range(100):\n",
        "  qfn = str(i+1)+\".pkl\"\n",
        "  qrep = load_qrep(qfn)\n",
        "  new_string = qrep['sql'].replace(' AS ', ' ')\n",
        "  tables = re.search('FROM(.*)WHERE', new_string).group(1)\n",
        "  counter = 0\n",
        "  table_size = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "  for word in imdb_table_sizes.keys():\n",
        "    if word in tables:\n",
        "      table_size[counter] = imdb_table_sizes[word]\n",
        "      counter += 1\n",
        "  jobm.append(new_string)\n",
        "  table_sizesm.append(table_size)\n",
        "  labelsm.append(float(sum([cardinality['actual'] for cardinality in (subplan['cardinality'] for subplan in qrep['subset_graph']['nodes'][:])])))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "j9Cy5qO7Rq_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the necessary modules"
      ],
      "metadata": {
        "id": "AumJaGNgrIUP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZSwwIFOS6hg"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic"
      ],
      "metadata": {
        "id": "hiVtkawhgFvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports\n",
        "import re\n",
        "import statistics\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "# obtaining input queries and labels (cardinality or cost)\n",
        "input_queries = job_light\n",
        "input_labels = labels #(\"labels\" for JOB-light cardinality and \"costs\" for JOB-light costs)\n",
        "\n",
        "# limiting the number of queries to be used in the model\n",
        "input_queries = input_queries#[:number of queries to be used]\n",
        "input_labels = input_labels#[:number of queries to be used]\n",
        "\n",
        "# tokenize dataset\n",
        "size = len(input_queries)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/bert_uncased_L-4_H-256_A-4', use_fast=True)\n",
        "encoded_corpus = tokenizer(text=input_queries[:int(size*0.8)],\n",
        "                            add_special_tokens=True,\n",
        "                            padding='longest',\n",
        "                            max_length=None,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt')\n",
        "input_ids = encoded_corpus['input_ids']\n",
        "attention_mask = encoded_corpus['attention_mask']\n",
        "\n",
        "encoded_corpus_test = tokenizer(text=input_queries[int(size*0.8):],\n",
        "                            add_special_tokens=True,\n",
        "                            padding='longest',\n",
        "                            max_length=None,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt')\n",
        "input_ids_test = encoded_corpus_test['input_ids']\n",
        "attention_mask_test = encoded_corpus_test['attention_mask']\n",
        "\n",
        "# split train, validation and test sets\n",
        "\n",
        "input_ids_val = input_ids[int(size*0.6):int(size*0.8)]\n",
        "input_ids_train = input_ids[:int(size*0.6)]\n",
        "\n",
        "attention_mask_val = attention_mask[int(size*0.6):int(size*0.8)]\n",
        "attention_mask_train = attention_mask[:int(size*0.6)]\n",
        "\n",
        "labels_test = input_labels[int(size*0.8):]\n",
        "labels_val = input_labels[int(size*0.6):int(size*0.8)]\n",
        "labels_train = input_labels[:int(size*0.6)]\n",
        "\n",
        "# calculate the mean of the target in the train data\n",
        "train_mean = statistics.mean(labels_train)\n",
        "\n",
        "# make dataloaders\n",
        "batch_size = 64\n",
        "\n",
        "def make_dataloader(inputs, masks, labels, batch_size):\n",
        "    labels_tens = torch.tensor(labels)\n",
        "    dataset = TensorDataset(inputs, masks, labels_tens)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    return dataloader\n",
        "\n",
        "train_dataloader = make_dataloader(input_ids_train, attention_mask_train, labels_train, batch_size)\n",
        "valid_dataloader = make_dataloader(input_ids_val, attention_mask_val, labels_val, batch_size)\n",
        "test_dataloader = make_dataloader(input_ids_test, attention_mask_test, labels_test, batch_size)\n",
        "\n",
        "# load BERT model and define our head\n",
        "config = BertConfig.from_pretrained('google/bert_uncased_L-4_H-256_A-4', num_labels=2, hidden_dropout_prob=0.3,\n",
        "                                    attention_probs_dropout_prob=0.3, output_attentions = False, output_hidden_states = True)\n",
        "\n",
        "class BertRegressor(nn.Module):\n",
        "    def __init__(self, drop_rate=0.3, config=config):\n",
        "\n",
        "        super(BertRegressor, self).__init__()\n",
        "        D_in, D_out = 256, 1\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('google/bert_uncased_L-4_H-256_A-4', config=config)\n",
        "        self.drop = nn.Dropout(drop_rate)\n",
        "        self.linear = nn.Linear(D_in, D_out)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, input_ids, attention_masks):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_masks)\n",
        "        hidden_states = outputs[2]\n",
        "        token_vecs = hidden_states[-2][:]\n",
        "        token_vecs = token_vecs.permute(1,0,2)\n",
        "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "        outputs = self.drop(sentence_embedding)\n",
        "        outputs = self.linear(outputs)\n",
        "        outputs = self.relu(outputs)\n",
        "        outputs = torch.squeeze(outputs, 1)\n",
        "        return outputs\n",
        "\n",
        "model = BertRegressor()\n",
        "\n",
        "# connect to GPU, if available\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# define optimizer, with scheduler for adapting learning rate\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 50 # define maximum number of epochs\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=steps)\n",
        "\n",
        "# define loss function\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# imports for statistics\n",
        "from sklearn.metrics import r2_score\n",
        "import statistics\n",
        "\n",
        "# define statistics\n",
        "list_r2=[]\n",
        "list_r2_train=[]\n",
        "list_qerror=[]\n",
        "list_qerror_train=[]\n",
        "list_sample_size=[]\n",
        "\n",
        "# function to predict using model\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    output = []\n",
        "    for batch in dataloader:\n",
        "        batch_inputs, batch_masks, batch_labels = tuple(b.to(device) for b in batch)\n",
        "        with torch.no_grad():\n",
        "            output += model(batch_inputs, batch_masks).view(1,-1).tolist()[0]\n",
        "    return output\n",
        "\n",
        "# function to train model\n",
        "def train(model, optimizer, scheduler, loss_function, epochs,\n",
        "          train_dataloader, valid_dataloader, device, clip_value=2):\n",
        "    best_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # print progress\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(\"-----\")\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        y_train = []\n",
        "        print(\"Training:\")\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            if step == 0:\n",
        "              print(\"1/\"+str(len(train_dataloader)), end='')\n",
        "            else:\n",
        "              print('\\b'*(len(str(step)) + len(str(len(train_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(train_dataloader)), end='')\n",
        "\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            predictions = model(b_input_ids, b_input_mask)\n",
        "            y_train += predictions.detach().cpu().tolist()\n",
        "            loss = loss_function(predictions, b_labels)\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "            # Update model's weights based on the gradients calculated during backpropagation\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+12))\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_dataloader)\n",
        "        print(f\"Training Loss: {avg_train_loss}\")\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        model.eval()\n",
        "        eval_loss = 0.0\n",
        "\n",
        "        y_val = []\n",
        "        with torch.no_grad():\n",
        "            print(\"Validating:\")\n",
        "            for step, batch in enumerate(valid_dataloader):\n",
        "                if step == 0:\n",
        "                  print(\"1/\"+str(len(valid_dataloader)), end='')\n",
        "                else:\n",
        "                  print('\\b'*(len(str(step)) + len(str(len(valid_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(valid_dataloader)), end='')\n",
        "\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "                eval_output =  model(b_input_ids, b_input_mask)\n",
        "                y_val += eval_output.detach().cpu().tolist()\n",
        "                loss = loss_function(eval_output, b_labels)\n",
        "\n",
        "                eval_loss += loss\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+14))\n",
        "\n",
        "        avg_eval_loss = eval_loss / len(valid_dataloader)\n",
        "        print(f\"Validation Loss: {avg_eval_loss}\")\n",
        "\n",
        "        # calculate extra statistics for the output\n",
        "\n",
        "        r2_train = r2_score(labels_train, y_train)\n",
        "        print(\"R² Train: \" + str(r2_train))\n",
        "        r2 = r2_score(labels_val, y_val)\n",
        "        print(\"R² Validation: \" + str(r2))\n",
        "\n",
        "        qerror_train = statistics.mean([max(labels_train[i], y_train[i])/min(labels_train[i], y_train[i] if y_train[i] > 0 else train_mean) for i in range(len(labels_train))])\n",
        "        print(\"Q-error Train: \" + str(qerror_train))\n",
        "        qerror = statistics.mean([max(labels_val[i], y_val[i])/min(labels_val[i], y_val[i] if y_val[i] > 0 else train_mean) for i in range(len(labels_val))])\n",
        "        print(\"Q-error Validation: \" + str(qerror))\n",
        "        print()\n",
        "\n",
        "        list_r2.append(r2)\n",
        "        list_r2_train.append(r2_train)\n",
        "        list_qerror.append(qerror)\n",
        "        list_qerror_train.append(qerror_train)\n",
        "        list_sample_size.append((epoch * 0.1 + 0.1))\n",
        "\n",
        "        # Early stopping based on validation loss\n",
        "        if avg_eval_loss < best_loss:\n",
        "            best_loss = avg_eval_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement == 5:\n",
        "                print(\"Validation loss has not improved in 5 epochs, stopping early.\")\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train model\n",
        "model = train(model, optimizer, scheduler, loss_function, epochs,\n",
        "              train_dataloader, valid_dataloader, device, clip_value=2)\n",
        "# Testing\n",
        "\n",
        "# define r2_score function\n",
        "def r2_score(outputs, labels):\n",
        "    labels_mean = statistics.mean(labels)\n",
        "    ss_tot = sum([(x-labels_mean)**2 for x in labels])\n",
        "    ss_res = sum([(x-y)**2 for x,y in zip(labels, outputs)])\n",
        "    r2 = (1 - (ss_res / (ss_tot if ss_tot != 0 else 1e-10)))\n",
        "    return r2\n",
        "\n",
        "# Measure performance on test set\n",
        "y_test = labels_test\n",
        "y_pred = predict(model, test_dataloader, device)\n",
        "\n",
        "# more imports for metrics to be used on test set\n",
        "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "import pandas as pd\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"MAE Test: \", mae)\n",
        "mdae = median_absolute_error(y_test, y_pred)\n",
        "print(\"MDAE Test: \", mdae)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"MSE Test: \", mse)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "print(\"MAPE Test: \", mape)\n",
        "mdape = ((pd.Series(y_test) - pd.Series(y_pred)) / pd.Series(y_test)).abs().median()\n",
        "print(\"MDAPE Test: \", mdape)\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "print(\"R² Test: \", r_squared)\n",
        "qerror = statistics.mean([max(y_test[i], y_pred[i] if y_pred[i] != 0 else train_mean)/min(y_test[i], y_pred[i] if y_pred[i] > 0 else train_mean) for i in range(len(y_test))])\n",
        "print(\"Q-error Test: \", qerror)"
      ],
      "metadata": {
        "id": "Pi1vjQbbxcfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#With standardization"
      ],
      "metadata": {
        "id": "JHmsY25WrZ3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports\n",
        "import re\n",
        "import statistics\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "# obtaining input queries and labels (cardinality or cost)\n",
        "input_queries = <put desired query dataset here>\n",
        "input_labels = <put desired target here> #(\"labels\" for JOB-light cardinality and \"costs\" for JOB-light costs)\n",
        "\n",
        "# limiting the number of queries to be used in the model\n",
        "input_queries = input_queries#[:number of queries to be used]\n",
        "input_labels = input_labels#[:number of queries to be used]\n",
        "\n",
        "# tokenize dataset\n",
        "size = len(input_queries)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/bert_uncased_L-4_H-256_A-4', use_fast=True)\n",
        "encoded_corpus = tokenizer(text=input_queries[:int(size*0.8)],\n",
        "                            add_special_tokens=True,\n",
        "                            padding='longest',\n",
        "                            max_length=None,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt')\n",
        "input_ids = encoded_corpus['input_ids']\n",
        "attention_mask = encoded_corpus['attention_mask']\n",
        "\n",
        "encoded_corpus_test = tokenizer(text=input_queries[int(size*0.8):],\n",
        "                            add_special_tokens=True,\n",
        "                            padding='longest',\n",
        "                            max_length=None,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt')\n",
        "input_ids_test = encoded_corpus_test['input_ids']\n",
        "attention_mask_test = encoded_corpus_test['attention_mask']\n",
        "\n",
        "# split train, validation and test sets\n",
        "\n",
        "input_ids_val = input_ids[int(size*0.6):int(size*0.8)]\n",
        "input_ids_train = input_ids[:int(size*0.6)]\n",
        "\n",
        "attention_mask_val = attention_mask[int(size*0.6):int(size*0.8)]\n",
        "attention_mask_train = attention_mask[:int(size*0.6)]\n",
        "\n",
        "labels_test = input_labels[int(size*0.8):]\n",
        "labels_val = input_labels[int(size*0.6):int(size*0.8)]\n",
        "labels_train = input_labels[:int(size*0.6)]\n",
        "\n",
        "# calculate the mean of the target in the train data\n",
        "train_mean = statistics.mean(labels_train)\n",
        "\n",
        "# standardize all data, based on the statistics of the train set\n",
        "class Standardizer:\n",
        "    def __init__(self):\n",
        "        self.means = 0\n",
        "        self.stds = 0\n",
        "\n",
        "    def standardize(self, train, validation, test):\n",
        "        train_stand = torch.tensor(train)\n",
        "        validation_stand = torch.tensor(validation)\n",
        "        test_stand = torch.tensor(test)\n",
        "\n",
        "        self.stds = train_stand.std(dim=0, keepdim=True)\n",
        "        train_stand = train_stand / self.stds\n",
        "        validation_stand = validation_stand / self.stds\n",
        "        test_stand = test_stand / self.stds\n",
        "\n",
        "        train_stand = train_stand.detach().cpu().tolist()\n",
        "        validation_stand = validation_stand.detach().cpu().tolist()\n",
        "        test_stand = test_stand.detach().cpu().tolist()\n",
        "        return train_stand, validation_stand, test_stand\n",
        "\n",
        "    def destandardize(self, data):\n",
        "        data_destand = torch.tensor(data)\n",
        "        data_destand = data_destand*self.stds\n",
        "        data_destand = data_destand.detach().cpu().tolist()\n",
        "        return data_destand\n",
        "\n",
        "standardizer = Standardizer()\n",
        "labels_train, labels_val, labels_test = standardizer.standardize(labels_train, labels_val, labels_test)\n",
        "\n",
        "# make dataloaders\n",
        "batch_size = 64\n",
        "\n",
        "def make_dataloader(inputs, masks, labels, batch_size):\n",
        "    labels_tens = torch.tensor(labels)\n",
        "    dataset = TensorDataset(inputs, masks, labels_tens)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    return dataloader\n",
        "\n",
        "train_dataloader = make_dataloader(input_ids_train, attention_mask_train, labels_train, batch_size)\n",
        "valid_dataloader = make_dataloader(input_ids_val, attention_mask_val, labels_val, batch_size)\n",
        "test_dataloader = make_dataloader(input_ids_test, attention_mask_test, labels_test, batch_size)\n",
        "\n",
        "labels_train = standardizer.destandardize(labels_train)\n",
        "labels_val = standardizer.destandardize(labels_val)\n",
        "labels_test = standardizer.destandardize(labels_test)\n",
        "\n",
        "# load BERT model and define our head\n",
        "config = BertConfig.from_pretrained('google/bert_uncased_L-4_H-256_A-4', num_labels=2, hidden_dropout_prob=0.3,\n",
        "                                    attention_probs_dropout_prob=0.3, output_attentions = False, output_hidden_states = True)\n",
        "\n",
        "class BertRegressor(nn.Module):\n",
        "    def __init__(self, drop_rate=0.3, config=config):\n",
        "\n",
        "        super(BertRegressor, self).__init__()\n",
        "        D_in, D_out = 256, 1\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('google/bert_uncased_L-4_H-256_A-4', config=config)\n",
        "        self.drop = nn.Dropout(drop_rate)\n",
        "        self.linear = nn.Linear(D_in, D_out)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, input_ids, attention_masks):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_masks)\n",
        "        hidden_states = outputs[2]\n",
        "        token_vecs = hidden_states[-2][:]\n",
        "        token_vecs = token_vecs.permute(1,0,2)\n",
        "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "        outputs = self.drop(sentence_embedding)\n",
        "        outputs = self.linear(outputs)\n",
        "        outputs = self.relu(outputs)\n",
        "        outputs = torch.squeeze(outputs, 1)\n",
        "        return outputs\n",
        "\n",
        "model = BertRegressor()\n",
        "\n",
        "# connect to GPU, if available\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# define optimizer, with scheduler for adapting learning rate\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 50 # define maximum number of epochs\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=steps)\n",
        "\n",
        "# define loss function\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# imports for statistics\n",
        "from sklearn.metrics import r2_score\n",
        "import statistics\n",
        "\n",
        "# define statistics\n",
        "list_r2=[]\n",
        "list_r2_train=[]\n",
        "list_qerror=[]\n",
        "list_qerror_train=[]\n",
        "list_sample_size=[]\n",
        "\n",
        "# function to predict using model\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    output = []\n",
        "    for batch in dataloader:\n",
        "        batch_inputs, batch_masks, batch_labels = tuple(b.to(device) for b in batch)\n",
        "        with torch.no_grad():\n",
        "            output += model(batch_inputs, batch_masks).view(1,-1).tolist()[0]\n",
        "    return output\n",
        "\n",
        "# function to train model\n",
        "def train(model, optimizer, scheduler, loss_function, epochs,\n",
        "          train_dataloader, valid_dataloader, device, clip_value=2):\n",
        "    best_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # print progress\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(\"-----\")\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        y_train = []\n",
        "        print(\"Training:\")\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            if step == 0:\n",
        "              print(\"1/\"+str(len(train_dataloader)), end='')\n",
        "            else:\n",
        "              print('\\b'*(len(str(step)) + len(str(len(train_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(train_dataloader)), end='')\n",
        "\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            predictions = model(b_input_ids, b_input_mask)\n",
        "            y_train += predictions.detach().cpu().tolist()\n",
        "            loss = loss_function(predictions, b_labels)\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "            # Update model's weights based on the gradients calculated during backpropagation\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+12))\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_dataloader)\n",
        "        print(f\"Training Loss: {avg_train_loss}\")\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        model.eval()\n",
        "        eval_loss = 0.0\n",
        "\n",
        "        y_val = []\n",
        "        with torch.no_grad():\n",
        "            print(\"Validating:\")\n",
        "            for step, batch in enumerate(valid_dataloader):\n",
        "                if step == 0:\n",
        "                  print(\"1/\"+str(len(valid_dataloader)), end='')\n",
        "                else:\n",
        "                  print('\\b'*(len(str(step)) + len(str(len(valid_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(valid_dataloader)), end='')\n",
        "\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "                eval_output =  model(b_input_ids, b_input_mask)\n",
        "                y_val += eval_output.detach().cpu().tolist()\n",
        "                loss = loss_function(eval_output, b_labels)\n",
        "\n",
        "                eval_loss += loss\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+14))\n",
        "\n",
        "        avg_eval_loss = eval_loss / len(valid_dataloader)\n",
        "        print(f\"Validation Loss: {avg_eval_loss}\")\n",
        "\n",
        "        # calculate extra statistics for the output\n",
        "        y_train = standardizer.destandardize(y_train)\n",
        "        y_val = standardizer.destandardize(y_val)\n",
        "\n",
        "        r2_train = r2_score(labels_train, y_train)\n",
        "        print(\"R² Train: \" + str(r2_train))\n",
        "        r2 = r2_score(labels_val, y_val)\n",
        "        print(\"R² Validation: \" + str(r2))\n",
        "\n",
        "        qerror_train = statistics.mean([max(labels_train[i], y_train[i])/min(labels_train[i], y_train[i] if y_train[i] > 0 else train_mean) for i in range(len(labels_train))])\n",
        "        print(\"Q-error Train: \" + str(qerror_train))\n",
        "        qerror = statistics.mean([max(labels_val[i], y_val[i])/min(labels_val[i], y_val[i] if y_val[i] > 0 else train_mean) for i in range(len(labels_val))])\n",
        "        print(\"Q-error Validation: \" + str(qerror))\n",
        "        print()\n",
        "\n",
        "        list_r2.append(r2)\n",
        "        list_r2_train.append(r2_train)\n",
        "        list_qerror.append(qerror)\n",
        "        list_qerror_train.append(qerror_train)\n",
        "        list_sample_size.append((epoch * 0.1 + 0.1))\n",
        "\n",
        "        # Early stopping based on validation loss\n",
        "        if avg_eval_loss < best_loss:\n",
        "            best_loss = avg_eval_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement == 5:\n",
        "                print(\"Validation loss has not improved in 5 epochs, stopping early.\")\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train model\n",
        "model = train(model, optimizer, scheduler, loss_function, epochs,\n",
        "              train_dataloader, valid_dataloader, device, clip_value=2)\n",
        "# Testing\n",
        "\n",
        "# define r2_score function\n",
        "def r2_score(outputs, labels):\n",
        "    labels_mean = statistics.mean(labels)\n",
        "    ss_tot = sum([(x-labels_mean)**2 for x in labels])\n",
        "    ss_res = sum([(x-y)**2 for x,y in zip(labels, outputs)])\n",
        "    r2 = (1 - (ss_res / (ss_tot if ss_tot != 0 else 1e-10)))\n",
        "    return r2\n",
        "\n",
        "# Measure performance on test set\n",
        "y_test = labels_test\n",
        "y_pred = predict(model, test_dataloader, device)\n",
        "y_pred = standardizer.destandardize(y_pred)\n",
        "\n",
        "# more imports for metrics to be used on test set\n",
        "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "import pandas as pd\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"MAE Test: \", mae)\n",
        "mdae = median_absolute_error(y_test, y_pred)\n",
        "print(\"MDAE Test: \", mdae)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"MSE Test: \", mse)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "print(\"MAPE Test: \", mape)\n",
        "mdape = ((pd.Series(y_test) - pd.Series(y_pred)) / pd.Series(y_test)).abs().median()\n",
        "print(\"MDAPE Test: \", mdape)\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "print(\"R² Test: \", r_squared)\n",
        "qerror = statistics.mean([max(y_test[i], y_pred[i] if y_pred[i] != 0 else train_mean)/min(y_test[i], y_pred[i] if y_pred[i] > 0 else train_mean) for i in range(len(y_test))])\n",
        "print(\"Q-error Test: \", qerror)"
      ],
      "metadata": {
        "id": "hoJSzb9CwwUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Standardization and preprocessing"
      ],
      "metadata": {
        "id": "3UGhrW9DreP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports\n",
        "import re\n",
        "import statistics\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "# function transforming numerical values in our dataset into tokens representing ranges\n",
        "def num_to_token(num):\n",
        "  if num < 1850:\n",
        "    return str(num)\n",
        "  if num < 2000:\n",
        "    return \"year1\"\n",
        "  elif num < 2010:\n",
        "    return \"year2\"\n",
        "  else:\n",
        "    return \"year3\"\n",
        "\n",
        "# function performing basic preprocessing\n",
        "def preprocess_query(query):\n",
        "  return re.sub(r\"(\\d+)\", lambda x: num_to_token(int(x.group(0))), query)\n",
        "\n",
        "# obtaining input queries and labels (cardinality or cost)\n",
        "input_queries = [preprocess_query(query) for query in <put desired query dataset here>]\n",
        "input_labels = <put desired target here> #(\"labels\" for JOB-light cardinality and \"costs\" for JOB-light costs)\n",
        "\n",
        "# limiting the number of queries to be used in the model\n",
        "input_queries = input_queries#[:number of queries to be used]\n",
        "input_labels = input_labels#[:number of queries to be used]\n",
        "\n",
        "# tokenize dataset\n",
        "size = len(input_queries)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/bert_uncased_L-4_H-256_A-4', use_fast=True)\n",
        "encoded_corpus = tokenizer(text=input_queries[:int(size*0.8)],\n",
        "                            add_special_tokens=True,\n",
        "                            padding='longest',\n",
        "                            max_length=None,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt')\n",
        "input_ids = encoded_corpus['input_ids']\n",
        "attention_mask = encoded_corpus['attention_mask']\n",
        "\n",
        "encoded_corpus_test = tokenizer(text=input_queries[int(size*0.8):],\n",
        "                            add_special_tokens=True,\n",
        "                            padding='longest',\n",
        "                            max_length=None,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt')\n",
        "input_ids_test = encoded_corpus_test['input_ids']\n",
        "attention_mask_test = encoded_corpus_test['attention_mask']\n",
        "\n",
        "# split train, validation and test sets\n",
        "\n",
        "input_ids_val = input_ids[int(size*0.6):int(size*0.8)]\n",
        "input_ids_train = input_ids[:int(size*0.6)]\n",
        "\n",
        "attention_mask_val = attention_mask[int(size*0.6):int(size*0.8)]\n",
        "attention_mask_train = attention_mask[:int(size*0.6)]\n",
        "\n",
        "labels_test = input_labels[int(size*0.8):]\n",
        "labels_val = input_labels[int(size*0.6):int(size*0.8)]\n",
        "labels_train = input_labels[:int(size*0.6)]\n",
        "\n",
        "# calculate the mean of the target in the train data\n",
        "train_mean = statistics.mean(labels_train)\n",
        "\n",
        "# standardize all data, based on the statistics of the train set\n",
        "class Standardizer:\n",
        "    def __init__(self):\n",
        "        self.means = 0\n",
        "        self.stds = 0\n",
        "\n",
        "    def standardize(self, train, validation, test):\n",
        "        train_stand = torch.tensor(train)\n",
        "        validation_stand = torch.tensor(validation)\n",
        "        test_stand = torch.tensor(test)\n",
        "\n",
        "        self.stds = train_stand.std(dim=0, keepdim=True)\n",
        "        train_stand = train_stand / self.stds\n",
        "        validation_stand = validation_stand / self.stds\n",
        "        test_stand = test_stand / self.stds\n",
        "\n",
        "        train_stand = train_stand.detach().cpu().tolist()\n",
        "        validation_stand = validation_stand.detach().cpu().tolist()\n",
        "        test_stand = test_stand.detach().cpu().tolist()\n",
        "        return train_stand, validation_stand, test_stand\n",
        "\n",
        "    def destandardize(self, data):\n",
        "        data_destand = torch.tensor(data)\n",
        "        data_destand = data_destand*self.stds\n",
        "        data_destand = data_destand.detach().cpu().tolist()\n",
        "        return data_destand\n",
        "\n",
        "standardizer = Standardizer()\n",
        "labels_train, labels_val, labels_test = standardizer.standardize(labels_train, labels_val, labels_test)\n",
        "\n",
        "# make dataloaders\n",
        "batch_size = 64\n",
        "\n",
        "def make_dataloader(inputs, masks, labels, batch_size):\n",
        "    labels_tens = torch.tensor(labels)\n",
        "    dataset = TensorDataset(inputs, masks, labels_tens)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    return dataloader\n",
        "\n",
        "train_dataloader = make_dataloader(input_ids_train, attention_mask_train, labels_train, batch_size)\n",
        "valid_dataloader = make_dataloader(input_ids_val, attention_mask_val, labels_val, batch_size)\n",
        "test_dataloader = make_dataloader(input_ids_test, attention_mask_test, labels_test, batch_size)\n",
        "\n",
        "labels_train = standardizer.destandardize(labels_train)\n",
        "labels_val = standardizer.destandardize(labels_val)\n",
        "labels_test = standardizer.destandardize(labels_test)\n",
        "\n",
        "# load BERT model and define our head\n",
        "config = BertConfig.from_pretrained('google/bert_uncased_L-4_H-256_A-4', num_labels=2, hidden_dropout_prob=0.3,\n",
        "                                    attention_probs_dropout_prob=0.3, output_attentions = False, output_hidden_states = True)\n",
        "\n",
        "class BertRegressor(nn.Module):\n",
        "    def __init__(self, drop_rate=0.3, config=config):\n",
        "\n",
        "        super(BertRegressor, self).__init__()\n",
        "        D_in, D_out = 256, 1\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('google/bert_uncased_L-4_H-256_A-4', config=config)\n",
        "        self.drop = nn.Dropout(drop_rate)\n",
        "        self.linear = nn.Linear(D_in, D_out)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, input_ids, attention_masks):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_masks)\n",
        "        hidden_states = outputs[2]\n",
        "        token_vecs = hidden_states[-2][:]\n",
        "        token_vecs = token_vecs.permute(1,0,2)\n",
        "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "        outputs = self.drop(sentence_embedding)\n",
        "        outputs = self.linear(outputs)\n",
        "        outputs = self.relu(outputs)\n",
        "        outputs = torch.squeeze(outputs, 1)\n",
        "        return outputs\n",
        "\n",
        "model = BertRegressor()\n",
        "\n",
        "# connect to GPU, if available\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# define optimizer, with scheduler for adapting learning rate\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 50 # define maximum number of epochs\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=steps)\n",
        "\n",
        "# define loss function\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# imports for statistics\n",
        "from sklearn.metrics import r2_score\n",
        "import statistics\n",
        "\n",
        "# define statistics\n",
        "list_r2=[]\n",
        "list_r2_train=[]\n",
        "list_qerror=[]\n",
        "list_qerror_train=[]\n",
        "list_sample_size=[]\n",
        "\n",
        "# function to predict using model\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    output = []\n",
        "    for batch in dataloader:\n",
        "        batch_inputs, batch_masks, batch_labels = tuple(b.to(device) for b in batch)\n",
        "        with torch.no_grad():\n",
        "            output += model(batch_inputs, batch_masks).view(1,-1).tolist()[0]\n",
        "    return output\n",
        "\n",
        "# function to train model\n",
        "def train(model, optimizer, scheduler, loss_function, epochs,\n",
        "          train_dataloader, valid_dataloader, device, clip_value=2):\n",
        "    best_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # print progress\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(\"-----\")\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        y_train = []\n",
        "        print(\"Training:\")\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            if step == 0:\n",
        "              print(\"1/\"+str(len(train_dataloader)), end='')\n",
        "            else:\n",
        "              print('\\b'*(len(str(step)) + len(str(len(train_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(train_dataloader)), end='')\n",
        "\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            predictions = model(b_input_ids, b_input_mask)\n",
        "            y_train += predictions.detach().cpu().tolist()\n",
        "            loss = loss_function(predictions, b_labels)\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "            # Update model's weights based on the gradients calculated during backpropagation\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+12))\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_dataloader)\n",
        "        print(f\"Training Loss: {avg_train_loss}\")\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        model.eval()\n",
        "        eval_loss = 0.0\n",
        "\n",
        "        y_val = []\n",
        "        with torch.no_grad():\n",
        "            print(\"Validating:\")\n",
        "            for step, batch in enumerate(valid_dataloader):\n",
        "                if step == 0:\n",
        "                  print(\"1/\"+str(len(valid_dataloader)), end='')\n",
        "                else:\n",
        "                  print('\\b'*(len(str(step)) + len(str(len(valid_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(valid_dataloader)), end='')\n",
        "\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "                eval_output =  model(b_input_ids, b_input_mask)\n",
        "                y_val += eval_output.detach().cpu().tolist()\n",
        "                loss = loss_function(eval_output, b_labels)\n",
        "\n",
        "                eval_loss += loss\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+14))\n",
        "\n",
        "        avg_eval_loss = eval_loss / len(valid_dataloader)\n",
        "        print(f\"Validation Loss: {avg_eval_loss}\")\n",
        "\n",
        "        # calculate extra statistics for the output\n",
        "        y_train = standardizer.destandardize(y_train)\n",
        "        y_val = standardizer.destandardize(y_val)\n",
        "\n",
        "        r2_train = r2_score(labels_train, y_train)\n",
        "        print(\"R² Train: \" + str(r2_train))\n",
        "        r2 = r2_score(labels_val, y_val)\n",
        "        print(\"R² Validation: \" + str(r2))\n",
        "\n",
        "        qerror_train = statistics.mean([max(labels_train[i], y_train[i])/min(labels_train[i], y_train[i] if y_train[i] > 0 else train_mean) for i in range(len(labels_train))])\n",
        "        print(\"Q-error Train: \" + str(qerror_train))\n",
        "        qerror = statistics.mean([max(labels_val[i], y_val[i])/min(labels_val[i], y_val[i] if y_val[i] > 0 else train_mean) for i in range(len(labels_val))])\n",
        "        print(\"Q-error Validation: \" + str(qerror))\n",
        "        print()\n",
        "\n",
        "        list_r2.append(r2)\n",
        "        list_r2_train.append(r2_train)\n",
        "        list_qerror.append(qerror)\n",
        "        list_qerror_train.append(qerror_train)\n",
        "        list_sample_size.append((epoch * 0.1 + 0.1))\n",
        "\n",
        "        # Early stopping based on validation loss\n",
        "        if avg_eval_loss < best_loss:\n",
        "            best_loss = avg_eval_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement == 5:\n",
        "                print(\"Validation loss has not improved in 5 epochs, stopping early.\")\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train model\n",
        "model = train(model, optimizer, scheduler, loss_function, epochs,\n",
        "              train_dataloader, valid_dataloader, device, clip_value=2)\n",
        "# Testing\n",
        "\n",
        "# define r2_score function\n",
        "def r2_score(outputs, labels):\n",
        "    labels_mean = statistics.mean(labels)\n",
        "    ss_tot = sum([(x-labels_mean)**2 for x in labels])\n",
        "    ss_res = sum([(x-y)**2 for x,y in zip(labels, outputs)])\n",
        "    r2 = (1 - (ss_res / (ss_tot if ss_tot != 0 else 1e-10)))\n",
        "    return r2\n",
        "\n",
        "# Measure performance on test set\n",
        "y_test = labels_test\n",
        "y_pred = predict(model, test_dataloader, device)\n",
        "y_pred = standardizer.destandardize(y_pred)\n",
        "\n",
        "# more imports for metrics to be used on test set\n",
        "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "import pandas as pd\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"MAE Test: \", mae)\n",
        "mdae = median_absolute_error(y_test, y_pred)\n",
        "print(\"MDAE Test: \", mdae)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"MSE Test: \", mse)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "print(\"MAPE Test: \", mape)\n",
        "mdape = ((pd.Series(y_test) - pd.Series(y_pred)) / pd.Series(y_test)).abs().median()\n",
        "print(\"MDAPE Test: \", mdape)\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "print(\"R² Test: \", r_squared)\n",
        "qerror = statistics.mean([max(y_test[i], y_pred[i] if y_pred[i] != 0 else train_mean)/min(y_test[i], y_pred[i] if y_pred[i] > 0 else train_mean) for i in range(len(y_test))])\n",
        "print(\"Q-error Test: \", qerror)"
      ],
      "metadata": {
        "id": "pPIAK-u1uatG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Truncation"
      ],
      "metadata": {
        "id": "fg4k7Y2xsB__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports\n",
        "import re\n",
        "import statistics\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "# function transforming numerical values in our dataset into tokens representing ranges\n",
        "def num_to_token(num):\n",
        "  if num < 1850:\n",
        "    return str(num)\n",
        "  if num < 2000:\n",
        "    return \"year1\"\n",
        "  elif num < 2010:\n",
        "    return \"year2\"\n",
        "  else:\n",
        "    return \"year3\"\n",
        "\n",
        "# function performing basic preprocessing\n",
        "def preprocess_query(query):\n",
        "  return re.sub(r\"(\\d+)\", lambda x: num_to_token(int(x.group(0))), query)\n",
        "\n",
        "# obtaining input queries and labels (cardinality or cost)\n",
        "input_queries = [preprocess_query(query) for query in <put desired query dataset here>]\n",
        "input_labels = <put desired target here> #(\"labels\" for JOB-light cardinality and \"costs\" for JOB-light costs)\n",
        "\n",
        "# limiting the number of queries to be used in the model\n",
        "input_queries = input_queries#[:number of queries to be used]\n",
        "input_labels = input_labels#[:number of queries to be used]\n",
        "\n",
        "# maximum query length before truncation, in number of tokens\n",
        "max_length = 256\n",
        "\n",
        "# tokenize dataset\n",
        "size = len(input_queries)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/bert_uncased_L-4_H-256_A-4', use_fast=True)\n",
        "encoded_corpus = tokenizer(text=input_queries[:int(size*0.8)],\n",
        "                            add_special_tokens=True,\n",
        "                            padding='max_length',\n",
        "                            truncation='longest_first',\n",
        "                            max_length=max_length,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt')\n",
        "input_ids = encoded_corpus['input_ids']\n",
        "attention_mask = encoded_corpus['attention_mask']\n",
        "\n",
        "encoded_corpus_test = tokenizer(text=input_queries[int(size*0.8):],\n",
        "                            add_special_tokens=True,\n",
        "                            padding='max_length',\n",
        "                            truncation='longest_first',\n",
        "                            max_length=max_length,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt')\n",
        "input_ids_test = encoded_corpus_test['input_ids']\n",
        "attention_mask_test = encoded_corpus_test['attention_mask']\n",
        "\n",
        "# split train, validation and test sets\n",
        "input_ids_val = input_ids[int(size*0.6):int(size*0.8)]\n",
        "input_ids_train = input_ids[:int(size*0.6)]\n",
        "\n",
        "attention_mask_val = attention_mask[int(size*0.6):int(size*0.8)]\n",
        "attention_mask_train = attention_mask[:int(size*0.6)]\n",
        "\n",
        "labels_test = input_labels[int(size*0.8):]\n",
        "labels_val = input_labels[int(size*0.6):int(size*0.8)]\n",
        "labels_train = input_labels[:int(size*0.6)]\n",
        "\n",
        "# calculate the mean of the target in the train data\n",
        "train_mean = statistics.mean(labels_train)\n",
        "\n",
        "# standardize all data, based on the statistics of the train set\n",
        "class Standardizer:\n",
        "    def __init__(self):\n",
        "        self.means = 0\n",
        "        self.stds = 0\n",
        "\n",
        "    def standardize(self, train, validation, test):\n",
        "        train_stand = torch.tensor(train)\n",
        "        validation_stand = torch.tensor(validation)\n",
        "        test_stand = torch.tensor(test)\n",
        "\n",
        "        self.stds = train_stand.std(dim=0, keepdim=True)\n",
        "        train_stand = train_stand / self.stds\n",
        "        validation_stand = validation_stand / self.stds\n",
        "        test_stand = test_stand / self.stds\n",
        "\n",
        "        train_stand = train_stand.detach().cpu().tolist()\n",
        "        validation_stand = validation_stand.detach().cpu().tolist()\n",
        "        test_stand = test_stand.detach().cpu().tolist()\n",
        "        return train_stand, validation_stand, test_stand\n",
        "\n",
        "    def destandardize(self, data):\n",
        "        data_destand = torch.tensor(data)\n",
        "        data_destand = data_destand*self.stds\n",
        "        data_destand = data_destand.detach().cpu().tolist()\n",
        "        return data_destand\n",
        "\n",
        "standardizer = Standardizer()\n",
        "labels_train, labels_val, labels_test = standardizer.standardize(labels_train, labels_val, labels_test)\n",
        "\n",
        "# make dataloaders\n",
        "batch_size = 64\n",
        "\n",
        "def make_dataloader(inputs, masks, labels, batch_size):\n",
        "    labels_tens = torch.tensor(labels)\n",
        "    dataset = TensorDataset(inputs, masks, labels_tens)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    return dataloader\n",
        "\n",
        "train_dataloader = make_dataloader(input_ids_train, attention_mask_train, labels_train, batch_size)\n",
        "valid_dataloader = make_dataloader(input_ids_val, attention_mask_val, labels_val, batch_size)\n",
        "test_dataloader = make_dataloader(input_ids_test, attention_mask_test, labels_test, batch_size)\n",
        "\n",
        "labels_train = standardizer.destandardize(labels_train)\n",
        "labels_val = standardizer.destandardize(labels_val)\n",
        "labels_test = standardizer.destandardize(labels_test)\n",
        "\n",
        "# load BERT model and define our head\n",
        "config = BertConfig.from_pretrained('google/bert_uncased_L-4_H-256_A-4', num_labels=2, hidden_dropout_prob=0.3,\n",
        "                                    attention_probs_dropout_prob=0.3, output_attentions = False, output_hidden_states = True)\n",
        "\n",
        "class BertRegressor(nn.Module):\n",
        "    def __init__(self, drop_rate=0.3, config=config):\n",
        "\n",
        "        super(BertRegressor, self).__init__()\n",
        "        D_in, D_out = 256, 1\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('google/bert_uncased_L-4_H-256_A-4', config=config)\n",
        "        self.drop = nn.Dropout(drop_rate)\n",
        "        self.linear = nn.Linear(D_in, D_out)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, input_ids, attention_masks):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_masks)\n",
        "        hidden_states = outputs[2]\n",
        "        token_vecs = hidden_states[-2][:]\n",
        "        token_vecs = token_vecs.permute(1,0,2)\n",
        "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "        outputs = self.drop(sentence_embedding)\n",
        "        outputs = self.linear(outputs)\n",
        "        outputs = self.relu(outputs)\n",
        "        outputs = torch.squeeze(outputs, 1)\n",
        "        return outputs\n",
        "\n",
        "model = BertRegressor()\n",
        "\n",
        "# connect to GPU, if available\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# define optimizer, with scheduler for adapting learning rate\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 50 # define maximum number of epochs\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=steps)\n",
        "\n",
        "# define loss function\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# imports for statistics\n",
        "from sklearn.metrics import r2_score\n",
        "import statistics\n",
        "\n",
        "# define statistics\n",
        "list_r2=[]\n",
        "list_r2_train=[]\n",
        "list_qerror=[]\n",
        "list_qerror_train=[]\n",
        "list_sample_size=[]\n",
        "\n",
        "# function to predict using model\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    output = []\n",
        "    for batch in dataloader:\n",
        "        batch_inputs, batch_masks, batch_labels = tuple(b.to(device) for b in batch)\n",
        "        with torch.no_grad():\n",
        "            output += model(batch_inputs, batch_masks).view(1,-1).tolist()[0]\n",
        "    return output\n",
        "\n",
        "# function to train model\n",
        "def train(model, optimizer, scheduler, loss_function, epochs,\n",
        "          train_dataloader, valid_dataloader, device, clip_value=2):\n",
        "    best_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # print progress\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(\"-----\")\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        y_train = []\n",
        "        print(\"Training:\")\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            if step == 0:\n",
        "              print(\"1/\"+str(len(train_dataloader)), end='')\n",
        "            else:\n",
        "              print('\\b'*(len(str(step)) + len(str(len(train_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(train_dataloader)), end='')\n",
        "\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            predictions = model(b_input_ids, b_input_mask)\n",
        "            y_train += predictions.detach().cpu().tolist()\n",
        "            loss = loss_function(predictions, b_labels)\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "            # Update model's weights based on the gradients calculated during backpropagation\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+12))\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_dataloader)\n",
        "        print(f\"Training Loss: {avg_train_loss}\")\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        model.eval()\n",
        "        eval_loss = 0.0\n",
        "\n",
        "        y_val = []\n",
        "        with torch.no_grad():\n",
        "            print(\"Validating:\")\n",
        "            for step, batch in enumerate(valid_dataloader):\n",
        "                if step == 0:\n",
        "                  print(\"1/\"+str(len(valid_dataloader)), end='')\n",
        "                else:\n",
        "                  print('\\b'*(len(str(step)) + len(str(len(valid_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(valid_dataloader)), end='')\n",
        "\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "                eval_output =  model(b_input_ids, b_input_mask)\n",
        "                y_val += eval_output.detach().cpu().tolist()\n",
        "                loss = loss_function(eval_output, b_labels)\n",
        "\n",
        "                eval_loss += loss\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+14))\n",
        "\n",
        "        avg_eval_loss = eval_loss / len(valid_dataloader)\n",
        "        print(f\"Validation Loss: {avg_eval_loss}\")\n",
        "\n",
        "        # calculate extra statistics for the output\n",
        "        y_train = standardizer.destandardize(y_train)\n",
        "        y_val = standardizer.destandardize(y_val)\n",
        "\n",
        "        r2_train = r2_score(labels_train, y_train)\n",
        "        print(\"R² Train: \" + str(r2_train))\n",
        "        r2 = r2_score(labels_val, y_val)\n",
        "        print(\"R² Validation: \" + str(r2))\n",
        "\n",
        "        qerror_train = statistics.mean([max(labels_train[i], y_train[i])/min(labels_train[i], y_train[i] if y_train[i] > 0 else train_mean) for i in range(len(labels_train))])\n",
        "        print(\"Q-error Train: \" + str(qerror_train))\n",
        "        qerror = statistics.mean([max(labels_val[i], y_val[i])/min(labels_val[i], y_val[i] if y_val[i] > 0 else train_mean) for i in range(len(labels_val))])\n",
        "        print(\"Q-error Validation: \" + str(qerror))\n",
        "        print()\n",
        "\n",
        "        list_r2.append(r2)\n",
        "        list_r2_train.append(r2_train)\n",
        "        list_qerror.append(qerror)\n",
        "        list_qerror_train.append(qerror_train)\n",
        "        list_sample_size.append((epoch * 0.1 + 0.1))\n",
        "\n",
        "        # Early stopping based on validation loss\n",
        "        if avg_eval_loss < best_loss:\n",
        "            best_loss = avg_eval_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement == 5:\n",
        "                print(\"Validation loss has not improved in 5 epochs, stopping early.\")\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train model\n",
        "model = train(model, optimizer, scheduler, loss_function, epochs,\n",
        "              train_dataloader, valid_dataloader, device, clip_value=2)\n",
        "# Testing\n",
        "\n",
        "# define r2_score function\n",
        "def r2_score(outputs, labels):\n",
        "    labels_mean = statistics.mean(labels)\n",
        "    ss_tot = sum([(x-labels_mean)**2 for x in labels])\n",
        "    ss_res = sum([(x-y)**2 for x,y in zip(labels, outputs)])\n",
        "    r2 = (1 - (ss_res / (ss_tot if ss_tot != 0 else 1e-10)))\n",
        "    return r2\n",
        "\n",
        "# Measure performance on test set\n",
        "y_test = labels_test\n",
        "y_pred = predict(model, test_dataloader, device)\n",
        "y_pred = standardizer.destandardize(y_pred)\n",
        "\n",
        "# more imports for metrics to be used on test set\n",
        "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "import pandas as pd\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"MAE Test: \", mae)\n",
        "mdae = median_absolute_error(y_test, y_pred)\n",
        "print(\"MDAE Test: \", mdae)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"MSE Test: \", mse)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "print(\"MAPE Test: \", mape)\n",
        "mdape = ((pd.Series(y_test) - pd.Series(y_pred)) / pd.Series(y_test)).abs().median()\n",
        "print(\"MDAPE Test: \", mdape)\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "print(\"R² Test: \", r_squared)\n",
        "qerror = statistics.mean([max(y_test[i], y_pred[i] if y_pred[i] != 0 else train_mean)/min(y_test[i], y_pred[i] if y_pred[i] > 0 else train_mean) for i in range(len(y_test))])\n",
        "print(\"Q-error Test: \", qerror)"
      ],
      "metadata": {
        "id": "tc991vO_nQ6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example"
      ],
      "metadata": {
        "id": "mLiTyEEyxliF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can take a long time to run the model on the full dataset without significant computational resources. An example of its output with all the data is the following:\n",
        "\n",
        "Epoch 1/50\n",
        "-----\n",
        "Training Loss: 0.6263792095476477\n",
        "\n",
        "Validation Loss: 0.5056275129318237\n",
        "\n",
        "R² Train: 0.3734703264157514\n",
        "\n",
        "R² Validation: 0.4855691207777376\n",
        "\n",
        "Q-error Train: 3.305453580852479\n",
        "\n",
        "Q-error Validation: 3.2212924519511623\n",
        "\n",
        "Epoch 2/50\n",
        "-----\n",
        "Training Loss: 0.4220415407911016\n",
        "\n",
        "Validation Loss: 0.4496574401855469\n",
        "\n",
        "R² Train: 0.5778525088854958\n",
        "\n",
        "R² Validation: 0.5427571535166092\n",
        "\n",
        "Q-error Train: 3.169342072568896\n",
        "\n",
        "Q-error Validation: 2.7176938294752224\n",
        "\n",
        "Epoch 3/50\n",
        "-----\n",
        "Training Loss: 0.3839462616944254\n",
        "\n",
        "Validation Loss: 0.36389797925949097\n",
        "\n",
        "R² Train: 0.6159775441705999\n",
        "\n",
        "R² Validation: 0.6305513043639062\n",
        "\n",
        "Q-error Train: 3.2330322042014576\n",
        "\n",
        "Q-error Validation: 3.1446021134992956\n",
        "\n",
        "Epoch 4/50\n",
        "-----\n",
        "Training Loss: 0.36595107361747714\n",
        "\n",
        "Validation Loss: 0.35223135352134705\n",
        "\n",
        "R² Train: 0.6339801306519363\n",
        "\n",
        "R² Validation: 0.6425509751170011\n",
        "\n",
        "Q-error Train: 2.5985835675678493\n",
        "\n",
        "Q-error Validation: 2.002590108586524\n",
        "\n",
        "Epoch 5/50\n",
        "-----\n",
        "Training Loss: 0.35313365140498565\n",
        "\n",
        "Validation Loss: 0.351616770029068\n",
        "\n",
        "R² Train: 0.6467960237482051\n",
        "\n",
        "R² Validation: 0.6430236526281277\n",
        "\n",
        "Q-error Train: 2.4418899832491476\n",
        "\n",
        "Q-error Validation: 2.0721219609033517\n",
        "\n",
        "Epoch 6/50\n",
        "-----\n",
        "Training Loss: 0.34719677209817423\n",
        "\n",
        "Validation Loss: 0.3376225531101227\n",
        "\n",
        "R² Train: 0.6527336546983005\n",
        "\n",
        "R² Validation: 0.6568136728588471\n",
        "\n",
        "Q-error Train: 2.451022208669985\n",
        "\n",
        "Q-error Validation: 1.9337873201502385\n",
        "\n",
        "Epoch 7/50\n",
        "-----\n",
        "Training Loss: 0.3302652337794821\n",
        "\n",
        "Validation Loss: 0.3134097754955292\n",
        "\n",
        "R² Train: 0.6696689056973306\n",
        "\n",
        "R² Validation: 0.6815532628657763\n",
        "\n",
        "Q-error Train: 2.8180485394905124\n",
        "\n",
        "Q-error Validation: 2.448247365505978\n",
        "\n",
        "Epoch 8/50\n",
        "-----\n",
        "Training Loss: 0.3203124474055074\n",
        "\n",
        "Validation Loss: 0.2965673506259918\n",
        "\n",
        "R² Train: 0.6796261704473632\n",
        "\n",
        "R² Validation: 0.6991716550770177\n",
        "\n",
        "Q-error Train: 2.471554392546991\n",
        "\n",
        "Q-error Validation: 2.424967714712766\n",
        "\n",
        "Epoch 9/50\n",
        "-----\n",
        "Training Loss: 0.30827594977881523\n",
        "\n",
        "Validation Loss: 0.27276352047920227\n",
        "\n",
        "R² Train: 0.6916648452912659\n",
        "\n",
        "R² Validation: 0.7234265002329364\n",
        "\n",
        "Q-error Train: 2.469976398412188\n",
        "\n",
        "Q-error Validation: 3.2448873674124457\n",
        "\n",
        "Epoch 10/50\n",
        "-----\n",
        "Training Loss: 0.29187094181629236\n",
        "\n",
        "Validation Loss: 0.2533513903617859\n",
        "\n",
        "R² Train: 0.7080649725790696\n",
        "\n",
        "R² Validation: 0.7432932783311175\n",
        "\n",
        "Q-error Train: 2.3000584370887167\n",
        "\n",
        "Q-error Validation: 1.8474129286696954\n",
        "\n",
        "Epoch 11/50\n",
        "-----\n",
        "Training Loss: 0.2709184946593246\n",
        "\n",
        "Validation Loss: 0.25212234258651733\n",
        "\n",
        "R² Train: 0.7290496653395091\n",
        "\n",
        "R² Validation: 0.7448058287925963\n",
        "\n",
        "Q-error Train: 2.4849044110244605\n",
        "\n",
        "Q-error Validation: 2.378509979477508\n",
        "\n",
        "Epoch 12/50\n",
        "-----\n",
        "Training Loss: 0.2536282257309982\n",
        "\n",
        "Validation Loss: 0.24619947373867035\n",
        "\n",
        "R² Train: 0.7463225243627698\n",
        "\n",
        "R² Validation: 0.7506716822939337\n",
        "\n",
        "Q-error Train: 2.169723929152888\n",
        "\n",
        "Q-error Validation: 1.9040034145887412\n",
        "\n",
        "Epoch 13/50\n",
        "-----\n",
        "Training Loss: 0.243370993328946\n",
        "\n",
        "Validation Loss: 0.22245445847511292\n",
        "\n",
        "R² Train: 0.7565825955331635\n",
        "\n",
        "R² Validation: 0.7749502104843015\n",
        "\n",
        "Q-error Train: 2.414969715068117\n",
        "\n",
        "Q-error Validation: 1.8254809175770956\n",
        "\n",
        "Epoch 14/50\n",
        "-----\n",
        "Training Loss: 0.23384952004723655\n",
        "\n",
        "Validation Loss: 0.2023150473833084\n",
        "\n",
        "R² Train: 0.7661062337462998\n",
        "\n",
        "R² Validation: 0.7956783709125155\n",
        "\n",
        "Q-error Train: 3.0248677334777523\n",
        "\n",
        "Q-error Validation: 3.656915663930562\n",
        "\n",
        "Epoch 15/50\n",
        "-----\n",
        "Training Loss: 0.2213311508995265\n",
        "\n",
        "Validation Loss: 0.21470589935779572\n",
        "\n",
        "R² Train: 0.7786274991226547\n",
        "\n",
        "R² Validation: 0.7832230373433039\n",
        "\n",
        "Q-error Train: 2.0874112375436655\n",
        "\n",
        "Q-error Validation: 4.561651962539864\n",
        "\n",
        "Epoch 16/50\n",
        "-----\n",
        "Training Loss: 0.2158280208413237\n",
        "\n",
        "Validation Loss: 0.20222777128219604\n",
        "\n",
        "R² Train: 0.7841293774685573\n",
        "\n",
        "R² Validation: 0.795802311081996\n",
        "\n",
        "Q-error Train: 2.1256507396741036\n",
        "\n",
        "Q-error Validation: 2.9716248277469877\n",
        "\n",
        "Epoch 17/50\n",
        "-----\n",
        "Training Loss: 0.21077621360240606\n",
        "\n",
        "Validation Loss: 0.20366927981376648\n",
        "\n",
        "R² Train: 0.7891971447411077\n",
        "\n",
        "R² Validation: 0.7942142900146185\n",
        "\n",
        "Q-error Train: 2.027032196453424\n",
        "\n",
        "Q-error Validation: 2.1816229754438976\n",
        "\n",
        "Epoch 18/50\n",
        "-----\n",
        "Training Loss: 0.19975997228634182\n",
        "\n",
        "Validation Loss: 0.18474720418453217\n",
        "\n",
        "R² Train: 0.8002006073163741\n",
        "\n",
        "R² Validation: 0.813373991547143\n",
        "\n",
        "Q-error Train: 2.0819505078696343\n",
        "\n",
        "Q-error Validation: 1.9553764353484455\n",
        "\n",
        "Epoch 19/50\n",
        "-----\n",
        "Training Loss: 0.19562459466382495\n",
        "\n",
        "Validation Loss: 0.1998322606086731\n",
        "\n",
        "R² Train: 0.8043368243517686\n",
        "\n",
        "R² Validation: 0.7983205145080368\n",
        "\n",
        "Q-error Train: 1.975451577415217\n",
        "\n",
        "Q-error Validation: 2.681332593141885\n",
        "\n",
        "Epoch 20/50\n",
        "-----\n",
        "Training Loss: 0.1936186439738485\n",
        "\n",
        "Validation Loss: 0.19062547385692596\n",
        "\n",
        "R² Train: 0.8063432195325804\n",
        "\n",
        "R² Validation: 0.8074037730704271\n",
        "\n",
        "Q-error Train: 2.0354150300759515\n",
        "\n",
        "Q-error Validation: 1.8139886174991744\n",
        "\n",
        "Epoch 21/50\n",
        "-----\n",
        "Training Loss: 0.18967999683078288\n",
        "\n",
        "Validation Loss: 0.19355548918247223\n",
        "\n",
        "R² Train: 0.8102884924396729\n",
        "\n",
        "R² Validation: 0.8043107299574423\n",
        "\n",
        "Q-error Train: 2.3548526556398603\n",
        "\n",
        "Q-error Validation: 1.96988455829343\n",
        "\n",
        "Epoch 22/50\n",
        "-----\n",
        "Training Loss: 0.18541605819218557\n",
        "\n",
        "Validation Loss: 0.18919335305690765\n",
        "\n",
        "R² Train: 0.8145394942226515\n",
        "\n",
        "R² Validation: 0.8087330692763872\n",
        "\n",
        "Q-error Train: 2.1490776616980494\n",
        "\n",
        "Q-error Validation: 1.8157930175078152\n",
        "\n",
        "Epoch 23/50\n",
        "-----\n",
        "Training Loss: 0.1795571636261758\n",
        "\n",
        "Validation Loss: 0.19267940521240234\n",
        "\n",
        "R² Train: 0.8204043709483474\n",
        "\n",
        "R² Validation: 0.8052432629516297\n",
        "\n",
        "Q-error Train: 2.2985047852906386\n",
        "\n",
        "Q-error Validation: 2.0560954384725605\n",
        "\n",
        "Validation loss has not improved in 5 epochs, stopping early.\n",
        "\n",
        "MAE Test:  13346795.93650263\n",
        "\n",
        "MDAE Test:  5532605.0\n",
        "\n",
        "MSE Test:  728272338886797.1\n",
        "\n",
        "MAPE Test:  0.543916324187827\n",
        "\n",
        "MDAPE Test:  0.3755998390401983\n",
        "\n",
        "R² Test:  0.7806363375028371\n",
        "\n",
        "Q-error Test:  2.213671969249493"
      ],
      "metadata": {
        "id": "cvuPILgDqNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#With table sizes added"
      ],
      "metadata": {
        "id": "8t-BJwr1y2xa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method (a)"
      ],
      "metadata": {
        "id": "a6onxYwfUeG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports\n",
        "import re\n",
        "import statistics\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "# function transforming numerical values in our dataset into tokens representing ranges\n",
        "def num_to_token(num):\n",
        "  if num < 1850:\n",
        "    return str(num)\n",
        "  if num < 2000:\n",
        "    return \"year1\"\n",
        "  elif num < 2010:\n",
        "    return \"year2\"\n",
        "  else:\n",
        "    return \"year3\"\n",
        "\n",
        "# function performing basic preprocessing\n",
        "def preprocess_query(query):\n",
        "  return re.sub(r\"(\\d+)\", lambda x: num_to_token(int(x.group(0))), query)\n",
        "\n",
        "input_queries = [preprocess_query(query) for query in job_light]#<put desired query dataset here>]\n",
        "input_labels = labels#<put desired target here> #(\"labels\" for JOB-light cardinality and \"costs\" for JOB-light costs)\n",
        "input_table_sizes = table_sizes#<put equivalent table sizes here>\n",
        "\n",
        "# limiting the number of queries to be used in the model\n",
        "input_queries = input_queries[:500]#[:number of queries to be used]\n",
        "input_labels = input_labels[:500]#[:number of queries to be used]\n",
        "input_table_sizes = input_table_sizes[:500]#[:number of queries to be used]\n",
        "\n",
        "# maximum query length before truncation, in number of tokens\n",
        "max_length = 256\n",
        "\n",
        "# tokenize dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/bert_uncased_L-4_H-256_A-4', use_fast=True)\n",
        "encoded_corpus = tokenizer(text=input_queries,\n",
        "                            add_special_tokens=True,\n",
        "                            padding='max_length',\n",
        "                            truncation='longest_first',\n",
        "                            max_length=max_length,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt')\n",
        "input_ids = encoded_corpus['input_ids']\n",
        "attention_mask = encoded_corpus['attention_mask']\n",
        "\n",
        "size = len(input_queries)\n",
        "\n",
        "# spliting data into train, validation and test sets\n",
        "\n",
        "input_ids_test = input_ids[int(size*0.8):]\n",
        "input_ids_val = input_ids[int(size*0.6):int(size*0.8)]\n",
        "input_ids_train = input_ids[:int(size*0.6)]\n",
        "\n",
        "attention_mask_test = attention_mask[int(size*0.8):]\n",
        "attention_mask_val = attention_mask[int(size*0.6):int(size*0.8)]\n",
        "attention_mask_train = attention_mask[:int(size*0.6)]\n",
        "\n",
        "labels_test = input_labels[int(size*0.8):]\n",
        "labels_val = input_labels[int(size*0.6):int(size*0.8)]\n",
        "labels_train = input_labels[:int(size*0.6)]\n",
        "\n",
        "table_sizes_test = input_table_sizes[int(size*0.8):]\n",
        "table_sizes_val = input_table_sizes[int(size*0.6):int(size*0.8)]\n",
        "table_sizes_train = input_table_sizes[:int(size*0.6)]\n",
        "\n",
        "# standardize all data, based on the statistics of the train set\n",
        "class Standardizer:\n",
        "    def __init__(self):\n",
        "        self.means = 0\n",
        "        self.stds = 0\n",
        "\n",
        "    def standardize_simple(self, train, validation, test):\n",
        "        train_stand = torch.tensor(train, dtype=torch.float)\n",
        "        validation_stand = torch.tensor(validation, dtype=torch.float)\n",
        "        test_stand = torch.tensor(test, dtype=torch.float)\n",
        "\n",
        "        stds = train_stand.std(dim=0, keepdim=True)\n",
        "        train_stand = train_stand / stds\n",
        "        validation_stand = validation_stand / stds\n",
        "        test_stand = test_stand / stds\n",
        "\n",
        "        train_stand = train_stand.detach().cpu().tolist()\n",
        "        validation_stand = validation_stand.detach().cpu().tolist()\n",
        "        test_stand = test_stand.detach().cpu().tolist()\n",
        "        return train_stand, validation_stand, test_stand\n",
        "\n",
        "    def standardize(self, train, validation, test):\n",
        "        train_stand = torch.tensor(train)\n",
        "        validation_stand = torch.tensor(validation)\n",
        "        test_stand = torch.tensor(test)\n",
        "\n",
        "        self.stds = train_stand.std(dim=0, keepdim=True)\n",
        "        train_stand = train_stand / self.stds\n",
        "        validation_stand = validation_stand / self.stds\n",
        "        test_stand = test_stand / self.stds\n",
        "\n",
        "        train_stand = train_stand.detach().cpu().tolist()\n",
        "        validation_stand = validation_stand.detach().cpu().tolist()\n",
        "        test_stand = test_stand.detach().cpu().tolist()\n",
        "        return train_stand, validation_stand, test_stand\n",
        "\n",
        "    def destandardize(self, data):\n",
        "        data_destand = torch.tensor(data)\n",
        "        data_destand = data_destand*self.stds\n",
        "        data_destand = data_destand.detach().cpu().tolist()\n",
        "        return data_destand\n",
        "\n",
        "standardizer = Standardizer()\n",
        "labels_train, labels_val, labels_test = standardizer.standardize(labels_train, labels_val, labels_test)\n",
        "table_sizes_train, table_sizes_val, table_sizes_test = standardizer.standardize_simple(table_sizes_train, table_sizes_val, table_sizes_test)\n",
        "\n",
        "# make dataloaders\n",
        "batch_size = 64\n",
        "\n",
        "def make_dataloader(inputs, masks, labels, table_sizes, batch_size):\n",
        "    labels_tens = torch.tensor(labels)\n",
        "    table_sizes_tens = torch.tensor(table_sizes, dtype=torch.float)\n",
        "    dataset = TensorDataset(inputs, masks, labels_tens, table_sizes_tens)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    return dataloader\n",
        "\n",
        "train_dataloader = make_dataloader(input_ids_train, attention_mask_train, labels_train, table_sizes_train, batch_size)\n",
        "valid_dataloader = make_dataloader(input_ids_val, attention_mask_val, labels_val, table_sizes_val, batch_size)\n",
        "test_dataloader = make_dataloader(input_ids_test, attention_mask_test, labels_test, table_sizes_test, batch_size)\n",
        "\n",
        "labels_train = standardizer.destandardize(labels_train)\n",
        "labels_val = standardizer.destandardize(labels_val)\n",
        "labels_test = standardizer.destandardize(labels_test)\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers import BertForSequenceClassification, BertConfig, BertModel\n",
        "\n",
        "# load bert model within our defined network\n",
        "config = BertConfig.from_pretrained('google/bert_uncased_L-4_H-256_A-4', num_labels=2, hidden_dropout_prob=0.3,\n",
        "                                    attention_probs_dropout_prob=0.3, output_attentions = False, output_hidden_states = True)\n",
        "\n",
        "\n",
        "class BertRegressor(nn.Module):\n",
        "\n",
        "    def __init__(self, drop_rate=0.3, config=config):\n",
        "\n",
        "        super(BertRegressor, self).__init__()\n",
        "        D_in, D_out = 256, 1\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('google/bert_uncased_L-4_H-256_A-4', config=config)\n",
        "        self.drop = nn.Dropout(drop_rate)\n",
        "        self.linear = nn.Linear(D_in, D_out)\n",
        "        self.linear2 = nn.Linear(D_out+4, D_out)  #+4 because we are adding table sizes\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, input_ids, attention_masks, table_sizes):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_masks)\n",
        "        hidden_states = outputs[2]\n",
        "        token_vecs = hidden_states[-2][:]\n",
        "        token_vecs = token_vecs.permute(1,0,2)\n",
        "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "        outputs = self.drop(sentence_embedding)\n",
        "        outputs = self.linear(outputs)\n",
        "        outputs = self.relu(outputs)\n",
        "        embedding_with_sizes = torch.cat(tensors=[outputs, table_sizes], dim=1)\n",
        "        outputs = self.linear2(embedding_with_sizes)\n",
        "        outputs = self.relu(outputs)\n",
        "        outputs = torch.squeeze(outputs, 1)\n",
        "        return outputs\n",
        "\n",
        "model = BertRegressor()\n",
        "\n",
        "# connect to gpu\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# define optimizer, with scheduler for adapting learning rate\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 50 # define maximum number of epochs\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=steps)\n",
        "\n",
        "# define loss function\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# imports for statistics\n",
        "from sklearn.metrics import r2_score\n",
        "import statistics\n",
        "\n",
        "# define statistics\n",
        "list_r2=[]\n",
        "list_r2_train=[]\n",
        "list_qerror=[]\n",
        "list_qerror_train=[]\n",
        "list_sample_size=[]\n",
        "\n",
        "# function to predict using model\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    output = []\n",
        "    for batch in dataloader:\n",
        "        batch_inputs, batch_masks, batch_labels, batch_table_sizes = tuple(b.to(device) for b in batch)\n",
        "        with torch.no_grad():\n",
        "            output += model(batch_inputs, batch_masks, batch_table_sizes).view(1,-1).tolist()[0]\n",
        "    return output\n",
        "\n",
        "# function to train model\n",
        "def train(model, optimizer, scheduler, loss_function, epochs,\n",
        "          train_dataloader, valid_dataloader, device, clip_value=2):\n",
        "    best_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(\"-----\")\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        y_train = []\n",
        "        print(\"Training:\")\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            if step == 0:\n",
        "              print(\"1/\"+str(len(train_dataloader)), end='')\n",
        "            else:\n",
        "              print('\\b'*(len(str(step)) + len(str(len(train_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(train_dataloader)), end='')\n",
        "\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels, b_table_sizes = batch\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            predictions = model(b_input_ids, b_input_mask, b_table_sizes)\n",
        "            y_train += predictions.detach().cpu().tolist()\n",
        "            loss = loss_function(predictions, b_labels)\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "            # Update model's weights based on the gradients calculated during backpropagation\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+12))\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_dataloader)\n",
        "        print(f\"Training Loss: {avg_train_loss}\")\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        model.eval()\n",
        "        eval_loss = 0.0\n",
        "\n",
        "        y_val = []\n",
        "        with torch.no_grad():\n",
        "            print(\"Validating:\")\n",
        "            for step, batch in enumerate(valid_dataloader):\n",
        "                if step == 0:\n",
        "                  print(\"1/\"+str(len(valid_dataloader)), end='')\n",
        "                else:\n",
        "                  print('\\b'*(len(str(step)) + len(str(len(valid_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(valid_dataloader)), end='')\n",
        "\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                b_input_ids, b_input_mask, b_labels, b_table_sizes = batch\n",
        "\n",
        "                eval_output =  model(b_input_ids, b_input_mask, b_table_sizes)\n",
        "                y_val += eval_output.detach().cpu().tolist()\n",
        "                loss = loss_function(eval_output, b_labels)\n",
        "\n",
        "                eval_loss += loss\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+14))\n",
        "\n",
        "        avg_eval_loss = eval_loss / len(valid_dataloader)\n",
        "        print(f\"Validation Loss: {avg_eval_loss}\")\n",
        "\n",
        "        # calculate extra statistics for visualisation\n",
        "        y_train = standardizer.destandardize(y_train)\n",
        "        y_val = standardizer.destandardize(y_val)\n",
        "\n",
        "        r2_train = r2_score(labels_train, y_train)\n",
        "        print(\"R² Train: \" + str(r2_train))\n",
        "        r2 = r2_score(labels_val, y_val)\n",
        "        print(\"R² Validation: \" + str(r2))\n",
        "\n",
        "        qerror_train = statistics.mean([max(labels_train[i], y_train[i])/min(labels_train[i], y_train[i] if y_train[i] > 0 else 1e-10) for i in range(len(labels_train))])\n",
        "        print(\"Q-error Train: \" + str(qerror_train))\n",
        "        qerror = statistics.mean([max(labels_val[i], y_val[i])/min(labels_val[i], y_val[i] if y_val[i] > 0 else 1e-10) for i in range(len(labels_val))])\n",
        "        print(\"Q-error Validation: \" + str(qerror))\n",
        "        print()\n",
        "\n",
        "        list_r2.append(r2)\n",
        "        list_r2_train.append(r2_train)\n",
        "        list_qerror.append(qerror)\n",
        "        list_qerror_train.append(qerror_train)\n",
        "        list_sample_size.append((epoch * 0.1 + 0.1))\n",
        "\n",
        "        # Early stopping based on validation loss\n",
        "        if avg_eval_loss < best_loss:\n",
        "            best_loss = avg_eval_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement == 5:\n",
        "                print(\"Validation loss has not improved in 5 epochs, stopping early.\")\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train model\n",
        "model = train(model, optimizer, scheduler, loss_function, epochs,\n",
        "              train_dataloader, valid_dataloader, device, clip_value=2)\n",
        "# Testing\n",
        "\n",
        "# define r2_score function\n",
        "def r2_score(outputs, labels):\n",
        "    labels_mean = statistics.mean(labels)\n",
        "    ss_tot = sum([(x-labels_mean)**2 for x in labels])\n",
        "    ss_res = sum([(x-y)**2 for x,y in zip(labels, outputs)])\n",
        "    r2 = (1 - (ss_res / (ss_tot if ss_tot != 0 else 1e-10)))\n",
        "    return r2\n",
        "\n",
        "# Measure performance on test set\n",
        "y_test = labels_test\n",
        "y_pred = predict(model, test_dataloader, device)\n",
        "y_pred = standardizer.destandardize(y_pred)\n",
        "\n",
        "# more imports for metrics to be used on test set\n",
        "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "import pandas as pd\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"MAE Test: \", mae)\n",
        "mdae = median_absolute_error(y_test, y_pred)\n",
        "print(\"MDAE Test: \", mdae)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"MSE Test: \", mse)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "print(\"MAPE Test: \", mape)\n",
        "mdape = ((pd.Series(y_test) - pd.Series(y_pred)) / pd.Series(y_test)).abs().median()\n",
        "print(\"MDAPE Test: \", mdape)\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "print(\"R² Test: \", r_squared)\n",
        "qerror = statistics.mean([max(y_test[i], y_pred[i] if y_pred[i] != 0 else train_mean)/min(y_test[i], y_pred[i] if y_pred[i] > 0 else train_mean) for i in range(len(y_test))])\n",
        "print(\"Q-error Test: \", qerror)"
      ],
      "metadata": {
        "id": "WsQRoSsgMdv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method (b)"
      ],
      "metadata": {
        "id": "k5G2yxjiM-rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports\n",
        "import re\n",
        "import statistics\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "# function transforming numerical values in our dataset into tokens representing ranges\n",
        "def num_to_token(num):\n",
        "  if num < 1850:\n",
        "    return str(num)\n",
        "  if num < 2000:\n",
        "    return \"year1\"\n",
        "  elif num < 2010:\n",
        "    return \"year2\"\n",
        "  else:\n",
        "    return \"year3\"\n",
        "\n",
        "# function performing basic preprocessing\n",
        "def preprocess_query(query):\n",
        "  return re.sub(r\"(\\d+)\", lambda x: num_to_token(int(x.group(0))), query)\n",
        "\n",
        "input_queries = [preprocess_query(query) for query in job_light]#<put desired query dataset here>]\n",
        "input_labels = labels#<put desired target here> #(\"labels\" for JOB-light cardinality and \"costs\" for JOB-light costs)\n",
        "input_table_sizes = table_sizes#<put equivalent table sizes here>\n",
        "\n",
        "# limiting the number of queries to be used in the model\n",
        "input_queries = input_queries[:500]#[:number of queries to be used]\n",
        "input_labels = input_labels[:500]#[:number of queries to be used]\n",
        "input_table_sizes = input_table_sizes[:500]#[:number of queries to be used]\n",
        "\n",
        "# maximum query length before truncation, in number of tokens\n",
        "max_length = 256\n",
        "\n",
        "# tokenize dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/bert_uncased_L-4_H-256_A-4', use_fast=True)\n",
        "encoded_corpus = tokenizer(text=input_queries,\n",
        "                            add_special_tokens=True,\n",
        "                            padding='max_length',\n",
        "                            truncation='longest_first',\n",
        "                            max_length=max_length,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt')\n",
        "input_ids = encoded_corpus['input_ids']\n",
        "attention_mask = encoded_corpus['attention_mask']\n",
        "\n",
        "size = len(input_queries)\n",
        "\n",
        "# spliting data into train, validation and test sets\n",
        "\n",
        "input_ids_test = input_ids[int(size*0.8):]\n",
        "input_ids_val = input_ids[int(size*0.6):int(size*0.8)]\n",
        "input_ids_train = input_ids[:int(size*0.6)]\n",
        "\n",
        "attention_mask_test = attention_mask[int(size*0.8):]\n",
        "attention_mask_val = attention_mask[int(size*0.6):int(size*0.8)]\n",
        "attention_mask_train = attention_mask[:int(size*0.6)]\n",
        "\n",
        "labels_test = input_labels[int(size*0.8):]\n",
        "labels_val = input_labels[int(size*0.6):int(size*0.8)]\n",
        "labels_train = input_labels[:int(size*0.6)]\n",
        "\n",
        "table_sizes_test = input_table_sizes[int(size*0.8):]\n",
        "table_sizes_val = input_table_sizes[int(size*0.6):int(size*0.8)]\n",
        "table_sizes_train = input_table_sizes[:int(size*0.6)]\n",
        "\n",
        "# standardize all data, based on the statistics of the train set\n",
        "class Standardizer:\n",
        "    def __init__(self):\n",
        "        self.means = 0\n",
        "        self.stds = 0\n",
        "\n",
        "    def standardize_simple(self, train, validation, test):\n",
        "        train_stand = torch.tensor(train, dtype=torch.float)\n",
        "        validation_stand = torch.tensor(validation, dtype=torch.float)\n",
        "        test_stand = torch.tensor(test, dtype=torch.float)\n",
        "\n",
        "        stds = train_stand.std(dim=0, keepdim=True)\n",
        "        train_stand = train_stand / stds\n",
        "        validation_stand = validation_stand / stds\n",
        "        test_stand = test_stand / stds\n",
        "\n",
        "        train_stand = train_stand.detach().cpu().tolist()\n",
        "        validation_stand = validation_stand.detach().cpu().tolist()\n",
        "        test_stand = test_stand.detach().cpu().tolist()\n",
        "        return train_stand, validation_stand, test_stand\n",
        "\n",
        "    def standardize(self, train, validation, test):\n",
        "        train_stand = torch.tensor(train)\n",
        "        validation_stand = torch.tensor(validation)\n",
        "        test_stand = torch.tensor(test)\n",
        "\n",
        "        self.stds = train_stand.std(dim=0, keepdim=True)\n",
        "        train_stand = train_stand / self.stds\n",
        "        validation_stand = validation_stand / self.stds\n",
        "        test_stand = test_stand / self.stds\n",
        "\n",
        "        train_stand = train_stand.detach().cpu().tolist()\n",
        "        validation_stand = validation_stand.detach().cpu().tolist()\n",
        "        test_stand = test_stand.detach().cpu().tolist()\n",
        "        return train_stand, validation_stand, test_stand\n",
        "\n",
        "    def destandardize(self, data):\n",
        "        data_destand = torch.tensor(data)\n",
        "        data_destand = data_destand*self.stds\n",
        "        data_destand = data_destand.detach().cpu().tolist()\n",
        "        return data_destand\n",
        "\n",
        "standardizer = Standardizer()\n",
        "labels_train, labels_val, labels_test = standardizer.standardize(labels_train, labels_val, labels_test)\n",
        "table_sizes_train, table_sizes_val, table_sizes_test = standardizer.standardize_simple(table_sizes_train, table_sizes_val, table_sizes_test)\n",
        "\n",
        "# make dataloaders\n",
        "batch_size = 64\n",
        "\n",
        "def make_dataloader(inputs, masks, labels, table_sizes, batch_size):\n",
        "    labels_tens = torch.tensor(labels)\n",
        "    table_sizes_tens = torch.tensor(table_sizes, dtype=torch.float)\n",
        "    dataset = TensorDataset(inputs, masks, labels_tens, table_sizes_tens)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    return dataloader\n",
        "\n",
        "train_dataloader = make_dataloader(input_ids_train, attention_mask_train, labels_train, table_sizes_train, batch_size)\n",
        "valid_dataloader = make_dataloader(input_ids_val, attention_mask_val, labels_val, table_sizes_val, batch_size)\n",
        "test_dataloader = make_dataloader(input_ids_test, attention_mask_test, labels_test, table_sizes_test, batch_size)\n",
        "\n",
        "labels_train = standardizer.destandardize(labels_train)\n",
        "labels_val = standardizer.destandardize(labels_val)\n",
        "labels_test = standardizer.destandardize(labels_test)\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers import BertForSequenceClassification, BertConfig, BertModel\n",
        "\n",
        "# load bert model within our defined network\n",
        "config = BertConfig.from_pretrained('google/bert_uncased_L-4_H-256_A-4', num_labels=2, hidden_dropout_prob=0.3,\n",
        "                                    attention_probs_dropout_prob=0.3, output_attentions = False, output_hidden_states = True)\n",
        "\n",
        "\n",
        "class BertRegressor(nn.Module):\n",
        "\n",
        "    def __init__(self, drop_rate=0.3, config=config):\n",
        "\n",
        "        super(BertRegressor, self).__init__()\n",
        "        D_in, D_out = 256, 1\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('google/bert_uncased_L-4_H-256_A-4', config=config)\n",
        "        self.drop = nn.Dropout(drop_rate)\n",
        "        self.linear = nn.Linear(D_in, D_out)\n",
        "        self.linear2 = nn.Linear(D_out+4, D_out+4)  #+4 because we are adding table sizes\n",
        "        self.linear3 = nn.Linear(D_out+4, D_out+4)  #+4 because we are adding table sizes\n",
        "        self.linear4 = nn.Linear(D_out+4, D_out)  #+4 because we are adding table sizes\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, input_ids, attention_masks, table_sizes):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_masks)\n",
        "        hidden_states = outputs[2]\n",
        "        token_vecs = hidden_states[-2][:]\n",
        "        token_vecs = token_vecs.permute(1,0,2)\n",
        "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "        outputs = self.drop(sentence_embedding)\n",
        "        outputs = self.linear(outputs)\n",
        "        outputs = self.relu(outputs)\n",
        "        embedding_with_sizes = torch.cat(tensors=[outputs, table_sizes], dim=1)\n",
        "        outputs = self.linear2(embedding_with_sizes)\n",
        "        outputs = self.relu(outputs)\n",
        "        outputs = self.linear3(embedding_with_sizes)\n",
        "        outputs = self.relu(outputs)\n",
        "        outputs = self.linear4(embedding_with_sizes)\n",
        "        outputs = self.relu(outputs)\n",
        "        outputs = torch.squeeze(outputs, 1)\n",
        "        return outputs\n",
        "\n",
        "model = BertRegressor()\n",
        "\n",
        "# connect to gpu\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# define optimizer, with scheduler for adapting learning rate\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 50 # define maximum number of epochs\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=steps)\n",
        "\n",
        "# define loss function\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# imports for statistics\n",
        "from sklearn.metrics import r2_score\n",
        "import statistics\n",
        "\n",
        "# define statistics\n",
        "list_r2=[]\n",
        "list_r2_train=[]\n",
        "list_qerror=[]\n",
        "list_qerror_train=[]\n",
        "list_sample_size=[]\n",
        "\n",
        "# function to predict using model\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    output = []\n",
        "    for batch in dataloader:\n",
        "        batch_inputs, batch_masks, batch_labels, batch_table_sizes = tuple(b.to(device) for b in batch)\n",
        "        with torch.no_grad():\n",
        "            output += model(batch_inputs, batch_masks, batch_table_sizes).view(1,-1).tolist()[0]\n",
        "    return output\n",
        "\n",
        "# function to train model\n",
        "def train(model, optimizer, scheduler, loss_function, epochs,\n",
        "          train_dataloader, valid_dataloader, device, clip_value=2):\n",
        "    best_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(\"-----\")\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        y_train = []\n",
        "        print(\"Training:\")\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            if step == 0:\n",
        "              print(\"1/\"+str(len(train_dataloader)), end='')\n",
        "            else:\n",
        "              print('\\b'*(len(str(step)) + len(str(len(train_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(train_dataloader)), end='')\n",
        "\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels, b_table_sizes = batch\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            predictions = model(b_input_ids, b_input_mask, b_table_sizes)\n",
        "            y_train += predictions.detach().cpu().tolist()\n",
        "            loss = loss_function(predictions, b_labels)\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "            # Update model's weights based on the gradients calculated during backpropagation\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+12))\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_dataloader)\n",
        "        print(f\"Training Loss: {avg_train_loss}\")\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        model.eval()\n",
        "        eval_loss = 0.0\n",
        "\n",
        "        y_val = []\n",
        "        with torch.no_grad():\n",
        "            print(\"Validating:\")\n",
        "            for step, batch in enumerate(valid_dataloader):\n",
        "                if step == 0:\n",
        "                  print(\"1/\"+str(len(valid_dataloader)), end='')\n",
        "                else:\n",
        "                  print('\\b'*(len(str(step)) + len(str(len(valid_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(valid_dataloader)), end='')\n",
        "\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                b_input_ids, b_input_mask, b_labels, b_table_sizes = batch\n",
        "\n",
        "                eval_output =  model(b_input_ids, b_input_mask, b_table_sizes)\n",
        "                y_val += eval_output.detach().cpu().tolist()\n",
        "                loss = loss_function(eval_output, b_labels)\n",
        "\n",
        "                eval_loss += loss\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+14))\n",
        "\n",
        "        avg_eval_loss = eval_loss / len(valid_dataloader)\n",
        "        print(f\"Validation Loss: {avg_eval_loss}\")\n",
        "\n",
        "        # calculate extra statistics for visualisation\n",
        "        y_train = standardizer.destandardize(y_train)\n",
        "        y_val = standardizer.destandardize(y_val)\n",
        "\n",
        "        r2_train = r2_score(labels_train, y_train)\n",
        "        print(\"R² Train: \" + str(r2_train))\n",
        "        r2 = r2_score(labels_val, y_val)\n",
        "        print(\"R² Validation: \" + str(r2))\n",
        "\n",
        "        qerror_train = statistics.mean([max(labels_train[i], y_train[i])/min(labels_train[i], y_train[i] if y_train[i] > 0 else 1e-10) for i in range(len(labels_train))])\n",
        "        print(\"Q-error Train: \" + str(qerror_train))\n",
        "        qerror = statistics.mean([max(labels_val[i], y_val[i])/min(labels_val[i], y_val[i] if y_val[i] > 0 else 1e-10) for i in range(len(labels_val))])\n",
        "        print(\"Q-error Validation: \" + str(qerror))\n",
        "        print()\n",
        "\n",
        "        list_r2.append(r2)\n",
        "        list_r2_train.append(r2_train)\n",
        "        list_qerror.append(qerror)\n",
        "        list_qerror_train.append(qerror_train)\n",
        "        list_sample_size.append((epoch * 0.1 + 0.1))\n",
        "\n",
        "        # Early stopping based on validation loss\n",
        "        if avg_eval_loss < best_loss:\n",
        "            best_loss = avg_eval_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement == 5:\n",
        "                print(\"Validation loss has not improved in 5 epochs, stopping early.\")\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train model\n",
        "model = train(model, optimizer, scheduler, loss_function, epochs,\n",
        "              train_dataloader, valid_dataloader, device, clip_value=2)\n",
        "# Testing\n",
        "\n",
        "# define r2_score function\n",
        "def r2_score(outputs, labels):\n",
        "    labels_mean = statistics.mean(labels)\n",
        "    ss_tot = sum([(x-labels_mean)**2 for x in labels])\n",
        "    ss_res = sum([(x-y)**2 for x,y in zip(labels, outputs)])\n",
        "    r2 = (1 - (ss_res / (ss_tot if ss_tot != 0 else 1e-10)))\n",
        "    return r2\n",
        "\n",
        "# Measure performance on test set\n",
        "y_test = labels_test\n",
        "y_pred = predict(model, test_dataloader, device)\n",
        "y_pred = standardizer.destandardize(y_pred)\n",
        "\n",
        "# more imports for metrics to be used on test set\n",
        "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "import pandas as pd\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"MAE Test: \", mae)\n",
        "mdae = median_absolute_error(y_test, y_pred)\n",
        "print(\"MDAE Test: \", mdae)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"MSE Test: \", mse)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "print(\"MAPE Test: \", mape)\n",
        "mdape = ((pd.Series(y_test) - pd.Series(y_pred)) / pd.Series(y_test)).abs().median()\n",
        "print(\"MDAPE Test: \", mdape)\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "print(\"R² Test: \", r_squared)\n",
        "qerror = statistics.mean([max(y_test[i], y_pred[i] if y_pred[i] != 0 else train_mean)/min(y_test[i], y_pred[i] if y_pred[i] > 0 else train_mean) for i in range(len(y_test))])\n",
        "print(\"Q-error Test: \", qerror)"
      ],
      "metadata": {
        "id": "zze41KLXMeNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method (c)"
      ],
      "metadata": {
        "id": "6VS5a5_nM2p_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports\n",
        "import re\n",
        "import statistics\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "# function transforming numerical values in our dataset into tokens representing ranges\n",
        "def num_to_token(num):\n",
        "  if num < 1850:\n",
        "    return str(num)\n",
        "  if num < 2000:\n",
        "    return \"year1\"\n",
        "  elif num < 2010:\n",
        "    return \"year2\"\n",
        "  else:\n",
        "    return \"year3\"\n",
        "\n",
        "# function performing basic preprocessing\n",
        "def preprocess_query(query):\n",
        "  return re.sub(r\"(\\d+)\", lambda x: num_to_token(int(x.group(0))), query)\n",
        "\n",
        "input_queries = [preprocess_query(query) for query in <put desired query dataset here>]\n",
        "input_labels = <put desired target here> #(\"labels\" for JOB-light cardinality and \"costs\" for JOB-light costs)\n",
        "input_table_sizes = put equivalent table sizes here>\n",
        "\n",
        "# limiting the number of queries to be used in the model\n",
        "input_queries = input_queries#[:number of queries to be used]\n",
        "input_labels = input_labels#[:number of queries to be used]\n",
        "input_table_sizes = input_table_sizes#[:number of queries to be used]\n",
        "\n",
        "# maximum query length before truncation, in number of tokens\n",
        "max_length = 256\n",
        "\n",
        "# tokenize dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/bert_uncased_L-4_H-256_A-4', use_fast=True)\n",
        "encoded_corpus = tokenizer(text=input_queries,\n",
        "                            add_special_tokens=True,\n",
        "                            padding='max_length',\n",
        "                            truncation='longest_first',\n",
        "                            max_length=max_length,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt')\n",
        "input_ids = encoded_corpus['input_ids']\n",
        "attention_mask = encoded_corpus['attention_mask']\n",
        "\n",
        "size = len(input_queries)\n",
        "\n",
        "# spliting data into train, validation and test sets\n",
        "\n",
        "input_ids_test = input_ids[int(size*0.8):]\n",
        "input_ids_val = input_ids[int(size*0.6):int(size*0.8)]\n",
        "input_ids_train = input_ids[:int(size*0.6)]\n",
        "\n",
        "attention_mask_test = attention_mask[int(size*0.8):]\n",
        "attention_mask_val = attention_mask[int(size*0.6):int(size*0.8)]\n",
        "attention_mask_train = attention_mask[:int(size*0.6)]\n",
        "\n",
        "labels_test = input_labels[int(size*0.8):]\n",
        "labels_val = input_labels[int(size*0.6):int(size*0.8)]\n",
        "labels_train = input_labels[:int(size*0.6)]\n",
        "\n",
        "table_sizes_test = input_table_sizes[int(size*0.8):]\n",
        "table_sizes_val = input_table_sizes[int(size*0.6):int(size*0.8)]\n",
        "table_sizes_train = input_table_sizes[:int(size*0.6)]\n",
        "\n",
        "# standardize all data, based on the statistics of the train set\n",
        "class Standardizer:\n",
        "    def __init__(self):\n",
        "        self.means = 0\n",
        "        self.stds = 0\n",
        "\n",
        "    def standardize_simple(self, train, validation, test):\n",
        "        train_stand = torch.tensor(train, dtype=torch.float)\n",
        "        validation_stand = torch.tensor(validation, dtype=torch.float)\n",
        "        test_stand = torch.tensor(test, dtype=torch.float)\n",
        "\n",
        "        stds = train_stand.std(dim=0, keepdim=True)\n",
        "        train_stand = train_stand / stds\n",
        "        validation_stand = validation_stand / stds\n",
        "        test_stand = test_stand / stds\n",
        "\n",
        "        train_stand = train_stand.detach().cpu().tolist()\n",
        "        validation_stand = validation_stand.detach().cpu().tolist()\n",
        "        test_stand = test_stand.detach().cpu().tolist()\n",
        "        return train_stand, validation_stand, test_stand\n",
        "\n",
        "    def standardize(self, train, validation, test):\n",
        "        train_stand = torch.tensor(train)\n",
        "        validation_stand = torch.tensor(validation)\n",
        "        test_stand = torch.tensor(test)\n",
        "\n",
        "        self.stds = train_stand.std(dim=0, keepdim=True)\n",
        "        train_stand = train_stand / self.stds\n",
        "        validation_stand = validation_stand / self.stds\n",
        "        test_stand = test_stand / self.stds\n",
        "\n",
        "        train_stand = train_stand.detach().cpu().tolist()\n",
        "        validation_stand = validation_stand.detach().cpu().tolist()\n",
        "        test_stand = test_stand.detach().cpu().tolist()\n",
        "        return train_stand, validation_stand, test_stand\n",
        "\n",
        "    def destandardize(self, data):\n",
        "        data_destand = torch.tensor(data)\n",
        "        data_destand = data_destand*self.stds\n",
        "        data_destand = data_destand.detach().cpu().tolist()\n",
        "        return data_destand\n",
        "\n",
        "standardizer = Standardizer()\n",
        "labels_train, labels_val, labels_test = standardizer.standardize(labels_train, labels_val, labels_test)\n",
        "table_sizes_train, table_sizes_val, table_sizes_test = standardizer.standardize_simple(table_sizes_train, table_sizes_val, table_sizes_test)\n",
        "\n",
        "# make dataloaders\n",
        "batch_size = 64\n",
        "\n",
        "def make_dataloader(inputs, masks, labels, table_sizes, batch_size):\n",
        "    labels_tens = torch.tensor(labels)\n",
        "    table_sizes_tens = torch.tensor(table_sizes, dtype=torch.float)\n",
        "    dataset = TensorDataset(inputs, masks, labels_tens, table_sizes_tens)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    return dataloader\n",
        "\n",
        "train_dataloader = make_dataloader(input_ids_train, attention_mask_train, labels_train, table_sizes_train, batch_size)\n",
        "valid_dataloader = make_dataloader(input_ids_val, attention_mask_val, labels_val, table_sizes_val, batch_size)\n",
        "test_dataloader = make_dataloader(input_ids_test, attention_mask_test, labels_test, table_sizes_test, batch_size)\n",
        "\n",
        "labels_train = standardizer.destandardize(labels_train)\n",
        "labels_val = standardizer.destandardize(labels_val)\n",
        "labels_test = standardizer.destandardize(labels_test)\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers import BertForSequenceClassification, BertConfig, BertModel\n",
        "\n",
        "# load bert model within our defined network\n",
        "config = BertConfig.from_pretrained('google/bert_uncased_L-4_H-256_A-4', num_labels=2, hidden_dropout_prob=0.3,\n",
        "                                    attention_probs_dropout_prob=0.3, output_attentions = False, output_hidden_states = True)\n",
        "\n",
        "class BertRegressor(nn.Module):\n",
        "\n",
        "    def __init__(self, drop_rate=0.3, config=config):\n",
        "\n",
        "        super(BertRegressor, self).__init__()\n",
        "        D_in, D_out = 256, 1\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('google/bert_uncased_L-4_H-256_A-4', config=config)\n",
        "        self.drop = nn.Dropout(drop_rate)\n",
        "        self.linear = nn.Linear(D_in+4, D_out)  #+4 because we are adding table sizes\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, input_ids, attention_masks, table_sizes):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_masks)\n",
        "        hidden_states = outputs[2]\n",
        "        token_vecs = hidden_states[-2][:]\n",
        "        token_vecs = token_vecs.permute(1,0,2)\n",
        "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "        outputs = self.drop(sentence_embedding)\n",
        "        embedding_with_sizes = torch.cat(tensors=[outputs, table_sizes], dim=1)\n",
        "        outputs = self.linear(embedding_with_sizes)\n",
        "        outputs = self.relu(outputs)\n",
        "        outputs = torch.squeeze(outputs, 1)\n",
        "        return outputs\n",
        "\n",
        "model = BertRegressor()\n",
        "\n",
        "# connect to gpu\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# define optimizer, with scheduler for adapting learning rate\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 50 # define maximum number of epochs\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=steps)\n",
        "\n",
        "# define loss function\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# imports for statistics\n",
        "from sklearn.metrics import r2_score\n",
        "import statistics\n",
        "\n",
        "# define statistics\n",
        "list_r2=[]\n",
        "list_r2_train=[]\n",
        "list_qerror=[]\n",
        "list_qerror_train=[]\n",
        "list_sample_size=[]\n",
        "\n",
        "# function to predict using model\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    output = []\n",
        "    for batch in dataloader:\n",
        "        batch_inputs, batch_masks, batch_labels, batch_table_sizes = tuple(b.to(device) for b in batch)\n",
        "        with torch.no_grad():\n",
        "            output += model(batch_inputs, batch_masks, batch_table_sizes).view(1,-1).tolist()[0]\n",
        "    return output\n",
        "\n",
        "# function to train model\n",
        "def train(model, optimizer, scheduler, loss_function, epochs,\n",
        "          train_dataloader, valid_dataloader, device, clip_value=2):\n",
        "    best_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(\"-----\")\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        y_train = []\n",
        "        print(\"Training:\")\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            if step == 0:\n",
        "              print(\"1/\"+str(len(train_dataloader)), end='')\n",
        "            else:\n",
        "              print('\\b'*(len(str(step)) + len(str(len(train_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(train_dataloader)), end='')\n",
        "\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels, b_table_sizes = batch\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            predictions = model(b_input_ids, b_input_mask, b_table_sizes)\n",
        "            y_train += predictions.detach().cpu().tolist()\n",
        "            loss = loss_function(predictions, b_labels)\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "            # Update model's weights based on the gradients calculated during backpropagation\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+12))\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_dataloader)\n",
        "        print(f\"Training Loss: {avg_train_loss}\")\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        model.eval()\n",
        "        eval_loss = 0.0\n",
        "\n",
        "        y_val = []\n",
        "        with torch.no_grad():\n",
        "            print(\"Validating:\")\n",
        "            for step, batch in enumerate(valid_dataloader):\n",
        "                if step == 0:\n",
        "                  print(\"1/\"+str(len(valid_dataloader)), end='')\n",
        "                else:\n",
        "                  print('\\b'*(len(str(step)) + len(str(len(valid_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(valid_dataloader)), end='')\n",
        "\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                b_input_ids, b_input_mask, b_labels, b_table_sizes = batch\n",
        "\n",
        "                eval_output =  model(b_input_ids, b_input_mask, b_table_sizes)\n",
        "                y_val += eval_output.detach().cpu().tolist()\n",
        "                loss = loss_function(eval_output, b_labels)\n",
        "\n",
        "                eval_loss += loss\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+14))\n",
        "\n",
        "        avg_eval_loss = eval_loss / len(valid_dataloader)\n",
        "        print(f\"Validation Loss: {avg_eval_loss}\")\n",
        "\n",
        "        # calculate extra statistics for visualisation\n",
        "        y_train = standardizer.destandardize(y_train)\n",
        "        y_val = standardizer.destandardize(y_val)\n",
        "\n",
        "        r2_train = r2_score(labels_train, y_train)\n",
        "        print(\"R² Train: \" + str(r2_train))\n",
        "        r2 = r2_score(labels_val, y_val)\n",
        "        print(\"R² Validation: \" + str(r2))\n",
        "\n",
        "        qerror_train = statistics.mean([max(labels_train[i], y_train[i])/min(labels_train[i], y_train[i] if y_train[i] > 0 else 1e-10) for i in range(len(labels_train))])\n",
        "        print(\"Q-error Train: \" + str(qerror_train))\n",
        "        qerror = statistics.mean([max(labels_val[i], y_val[i])/min(labels_val[i], y_val[i] if y_val[i] > 0 else 1e-10) for i in range(len(labels_val))])\n",
        "        print(\"Q-error Validation: \" + str(qerror))\n",
        "        print()\n",
        "\n",
        "        list_r2.append(r2)\n",
        "        list_r2_train.append(r2_train)\n",
        "        list_qerror.append(qerror)\n",
        "        list_qerror_train.append(qerror_train)\n",
        "        list_sample_size.append((epoch * 0.1 + 0.1))\n",
        "\n",
        "        # Early stopping based on validation loss\n",
        "        if avg_eval_loss < best_loss:\n",
        "            best_loss = avg_eval_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement == 5:\n",
        "                print(\"Validation loss has not improved in 5 epochs, stopping early.\")\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train model\n",
        "model = train(model, optimizer, scheduler, loss_function, epochs,\n",
        "              train_dataloader, valid_dataloader, device, clip_value=2)\n",
        "# Testing\n",
        "\n",
        "# define r2_score function\n",
        "def r2_score(outputs, labels):\n",
        "    labels_mean = statistics.mean(labels)\n",
        "    ss_tot = sum([(x-labels_mean)**2 for x in labels])\n",
        "    ss_res = sum([(x-y)**2 for x,y in zip(labels, outputs)])\n",
        "    r2 = (1 - (ss_res / (ss_tot if ss_tot != 0 else 1e-10)))\n",
        "    return r2\n",
        "\n",
        "# Measure performance on test set\n",
        "y_test = labels_test\n",
        "y_pred = predict(model, test_dataloader, device)\n",
        "y_pred = standardizer.destandardize(y_pred)\n",
        "\n",
        "# more imports for metrics to be used on test set\n",
        "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "import pandas as pd\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"MAE Test: \", mae)\n",
        "mdae = median_absolute_error(y_test, y_pred)\n",
        "print(\"MDAE Test: \", mdae)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"MSE Test: \", mse)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "print(\"MAPE Test: \", mape)\n",
        "mdape = ((pd.Series(y_test) - pd.Series(y_pred)) / pd.Series(y_test)).abs().median()\n",
        "print(\"MDAPE Test: \", mdape)\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "print(\"R² Test: \", r_squared)\n",
        "qerror = statistics.mean([max(y_test[i], y_pred[i] if y_pred[i] != 0 else train_mean)/min(y_test[i], y_pred[i] if y_pred[i] > 0 else train_mean) for i in range(len(y_test))])\n",
        "print(\"Q-error Test: \", qerror)"
      ],
      "metadata": {
        "id": "IOzbOihaLGAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Only table sizes"
      ],
      "metadata": {
        "id": "nExHKj1ay9-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports\n",
        "import re\n",
        "import statistics\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "input_queries = [preprocess_query(query) for query in <put desired query dataset here>]\n",
        "input_labels = <put desired target here> #(\"labels\" for JOB-light cardinality and \"costs\" for JOB-light costs)\n",
        "input_table_sizes = <put equivalent table sizes here>\n",
        "\n",
        "# limiting the number of queries to be used in the model\n",
        "input_queries = input_queries#[:number of queries to be used]\n",
        "input_labels = input_labels#[:number of queries to be used]\n",
        "input_table_sizes = input_table_sizes#[:number of queries to be used]\n",
        "\n",
        "size = len(input_queries)\n",
        "\n",
        "# spliting data into train, validation and test sets\n",
        "\n",
        "labels_test = input_labels[int(size*0.8):]\n",
        "labels_val = input_labels[int(size*0.6):int(size*0.8)]\n",
        "labels_train = input_labels[:int(size*0.6)]\n",
        "\n",
        "table_sizes_test = input_table_sizes[int(size*0.8):]\n",
        "table_sizes_val = input_table_sizes[int(size*0.6):int(size*0.8)]\n",
        "table_sizes_train = input_table_sizes[:int(size*0.6)]\n",
        "\n",
        "# standardize all data, based on the statistics of the train set\n",
        "class Standardizer:\n",
        "    def __init__(self):\n",
        "        self.means = 0\n",
        "        self.stds = 0\n",
        "\n",
        "    def standardize_simple(self, train, validation, test):\n",
        "        train_stand = torch.tensor(train, dtype=torch.float)\n",
        "        validation_stand = torch.tensor(validation, dtype=torch.float)\n",
        "        test_stand = torch.tensor(test, dtype=torch.float)\n",
        "\n",
        "        stds = train_stand.std(dim=0, keepdim=True)\n",
        "        train_stand = train_stand / stds\n",
        "        validation_stand = validation_stand / stds\n",
        "        test_stand = test_stand / stds\n",
        "\n",
        "        train_stand = train_stand.detach().cpu().tolist()\n",
        "        validation_stand = validation_stand.detach().cpu().tolist()\n",
        "        test_stand = test_stand.detach().cpu().tolist()\n",
        "        return train_stand, validation_stand, test_stand\n",
        "\n",
        "    def standardize(self, train, validation, test):\n",
        "        train_stand = torch.tensor(train)\n",
        "        validation_stand = torch.tensor(validation)\n",
        "        test_stand = torch.tensor(test)\n",
        "\n",
        "        self.stds = train_stand.std(dim=0, keepdim=True)\n",
        "        train_stand = train_stand / self.stds\n",
        "        validation_stand = validation_stand / self.stds\n",
        "        test_stand = test_stand / self.stds\n",
        "\n",
        "        train_stand = train_stand.detach().cpu().tolist()\n",
        "        validation_stand = validation_stand.detach().cpu().tolist()\n",
        "        test_stand = test_stand.detach().cpu().tolist()\n",
        "        return train_stand, validation_stand, test_stand\n",
        "\n",
        "    def destandardize(self, data):\n",
        "        data_destand = torch.tensor(data)\n",
        "        data_destand = data_destand*self.stds\n",
        "        data_destand = data_destand.detach().cpu().tolist()\n",
        "        return data_destand\n",
        "\n",
        "standardizer = Standardizer()\n",
        "labels_train, labels_val, labels_test = standardizer.standardize(labels_train, labels_val, labels_test)\n",
        "table_sizes_train, table_sizes_val, table_sizes_test = standardizer.standardize_simple(table_sizes_train, table_sizes_val, table_sizes_test)\n",
        "\n",
        "# make dataloaders\n",
        "batch_size = 64\n",
        "\n",
        "def make_dataloader(labels, table_sizes, batch_size):\n",
        "    labels_tens = torch.tensor(labels)\n",
        "    table_sizes_tens = torch.tensor(table_sizes, dtype=torch.float)\n",
        "    dataset = TensorDataset(labels_tens, table_sizes_tens)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    return dataloader\n",
        "\n",
        "train_dataloader = make_dataloader(labels_train, table_sizes_train, batch_size)\n",
        "valid_dataloader = make_dataloader(labels_val, table_sizes_val, batch_size)\n",
        "test_dataloader = make_dataloader(labels_test, table_sizes_test, batch_size)\n",
        "\n",
        "labels_train = standardizer.destandardize(labels_train)\n",
        "labels_val = standardizer.destandardize(labels_val)\n",
        "labels_test = standardizer.destandardize(labels_test)\n",
        "\n",
        "class Regressor(nn.Module):\n",
        "\n",
        "    def __init__(self, drop_rate=0.3, config=config):\n",
        "\n",
        "        super(Regressor, self).__init__()\n",
        "        D_in, D_out = 4, 1\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('google/bert_uncased_L-4_H-256_A-4', config=config)\n",
        "        self.drop = nn.Dropout(drop_rate)\n",
        "        self.linear = nn.Linear(D_in, D_in)\n",
        "        self.linear2 = nn.Linear(D_in, D_in)\n",
        "        self.linear3 = nn.Linear(D_in, D_in)\n",
        "        self.linear4 = nn.Linear(D_in, D_out)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, table_sizes):\n",
        "        outputs = self.linear(table_sizes)\n",
        "        outputs = self.relu(outputs)\n",
        "        outputs = self.linear2(table_sizes)\n",
        "        outputs = self.relu(outputs)\n",
        "        outputs = self.linear3(table_sizes)\n",
        "        outputs = self.relu(outputs)\n",
        "        outputs = self.linear4(table_sizes)\n",
        "        outputs = self.relu(outputs)\n",
        "        outputs = torch.squeeze(outputs, 1)\n",
        "        return outputs\n",
        "\n",
        "model = Regressor()\n",
        "\n",
        "# connect to gpu\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# define optimizer, with scheduler for adapting learning rate\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 50 # define maximum number of epochs\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=steps)\n",
        "\n",
        "# define loss function\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# imports for statistics\n",
        "from sklearn.metrics import r2_score\n",
        "import statistics\n",
        "\n",
        "# define statistics\n",
        "list_r2=[]\n",
        "list_r2_train=[]\n",
        "list_qerror=[]\n",
        "list_qerror_train=[]\n",
        "list_sample_size=[]\n",
        "\n",
        "# function to predict using model\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    output = []\n",
        "    for batch in dataloader:\n",
        "        batch_labels, batch_table_sizes = tuple(b.to(device) for b in batch)\n",
        "        with torch.no_grad():\n",
        "            output += model(batch_table_sizes).view(1,-1).tolist()[0]\n",
        "    return output\n",
        "\n",
        "# function to train model\n",
        "def train(model, optimizer, scheduler, loss_function, epochs,\n",
        "          train_dataloader, valid_dataloader, device, clip_value=2):\n",
        "    best_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(\"-----\")\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        y_train = []\n",
        "        print(\"Training:\")\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            if step == 0:\n",
        "              print(\"1/\"+str(len(train_dataloader)), end='')\n",
        "            else:\n",
        "              print('\\b'*(len(str(step)) + len(str(len(train_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(train_dataloader)), end='')\n",
        "\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_labels, b_table_sizes = batch\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            predictions = model(b_table_sizes)\n",
        "            y_train += predictions.detach().cpu().tolist()\n",
        "            loss = loss_function(predictions, b_labels)\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "            # Update model's weights based on the gradients calculated during backpropagation\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+12))\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_dataloader)\n",
        "        print(f\"Training Loss: {avg_train_loss}\")\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        model.eval()\n",
        "        eval_loss = 0.0\n",
        "\n",
        "        y_val = []\n",
        "        with torch.no_grad():\n",
        "            print(\"Validating:\")\n",
        "            for step, batch in enumerate(valid_dataloader):\n",
        "                if step == 0:\n",
        "                  print(\"1/\"+str(len(valid_dataloader)), end='')\n",
        "                else:\n",
        "                  print('\\b'*(len(str(step)) + len(str(len(valid_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(valid_dataloader)), end='')\n",
        "\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                b_labels, b_table_sizes = batch\n",
        "\n",
        "                eval_output =  model(b_table_sizes)\n",
        "                y_val += eval_output.detach().cpu().tolist()\n",
        "                loss = loss_function(eval_output, b_labels)\n",
        "\n",
        "                eval_loss += loss\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+14))\n",
        "\n",
        "        avg_eval_loss = eval_loss / len(valid_dataloader)\n",
        "        print(f\"Validation Loss: {avg_eval_loss}\")\n",
        "\n",
        "        # calculate extra statistics for visualisation\n",
        "        y_train = standardizer.destandardize(y_train)\n",
        "        y_val = standardizer.destandardize(y_val)\n",
        "\n",
        "        r2_train = r2_score(labels_train, y_train)\n",
        "        print(\"R² Train: \" + str(r2_train))\n",
        "        r2 = r2_score(labels_val, y_val)\n",
        "        print(\"R² Validation: \" + str(r2))\n",
        "\n",
        "        qerror_train = statistics.mean([max(labels_train[i], y_train[i])/min(labels_train[i], y_train[i] if y_train[i] > 0 else 1e-10) for i in range(len(labels_train))])\n",
        "        print(\"Q-error Train: \" + str(qerror_train))\n",
        "        qerror = statistics.mean([max(labels_val[i], y_val[i])/min(labels_val[i], y_val[i] if y_val[i] > 0 else 1e-10) for i in range(len(labels_val))])\n",
        "        print(\"Q-error Validation: \" + str(qerror))\n",
        "        print()\n",
        "\n",
        "        list_r2.append(r2)\n",
        "        list_r2_train.append(r2_train)\n",
        "        list_qerror.append(qerror)\n",
        "        list_qerror_train.append(qerror_train)\n",
        "        list_sample_size.append((epoch * 0.1 + 0.1))\n",
        "\n",
        "        # Early stopping based on validation loss\n",
        "        if avg_eval_loss < best_loss:\n",
        "            best_loss = avg_eval_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement == 5:\n",
        "                print(\"Validation loss has not improved in 5 epochs, stopping early.\")\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train model\n",
        "model = train(model, optimizer, scheduler, loss_function, epochs,\n",
        "              train_dataloader, valid_dataloader, device, clip_value=2)\n",
        "# Testing\n",
        "\n",
        "# define r2_score function\n",
        "def r2_score(outputs, labels):\n",
        "    labels_mean = statistics.mean(labels)\n",
        "    ss_tot = sum([(x-labels_mean)**2 for x in labels])\n",
        "    ss_res = sum([(x-y)**2 for x,y in zip(labels, outputs)])\n",
        "    r2 = (1 - (ss_res / (ss_tot if ss_tot != 0 else 1e-10)))\n",
        "    return r2\n",
        "\n",
        "# Measure performance on test set\n",
        "y_test = labels_test\n",
        "y_pred = predict(model, test_dataloader, device)\n",
        "y_pred = standardizer.destandardize(y_pred)\n",
        "\n",
        "# more imports for metrics to be used on test set\n",
        "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "import pandas as pd\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"MAE Test: \", mae)\n",
        "mdae = median_absolute_error(y_test, y_pred)\n",
        "print(\"MDAE Test: \", mdae)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"MSE Test: \", mse)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "print(\"MAPE Test: \", mape)\n",
        "mdape = ((pd.Series(y_test) - pd.Series(y_pred)) / pd.Series(y_test)).abs().median()\n",
        "print(\"MDAPE Test: \", mdape)\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "print(\"R² Test: \", r_squared)\n",
        "qerror = statistics.mean([max(y_test[i], y_pred[i] if y_pred[i] != 0 else train_mean)/min(y_test[i], y_pred[i] if y_pred[i] > 0 else train_mean) for i in range(len(y_test))])\n",
        "print(\"Q-error Test: \", qerror)"
      ],
      "metadata": {
        "id": "1Oob7ACMJSeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Limited syntax analysis"
      ],
      "metadata": {
        "id": "uVIBUPCDy_0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports\n",
        "import re\n",
        "import statistics\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "# function transforming numerical values in our dataset into tokens representing ranges\n",
        "def num_to_token(num):\n",
        "  if num < 1850:\n",
        "    return str(num)\n",
        "  if num < 2000:\n",
        "    return \"year1\"\n",
        "  elif num < 2010:\n",
        "    return \"year2\"\n",
        "  else:\n",
        "    return \"year3\"\n",
        "\n",
        "# function performing basic preprocessing\n",
        "def preprocess_query(query):\n",
        "  return re.sub(r\"(\\d+)\", lambda x: num_to_token(int(x.group(0))), query)\n",
        "\n",
        "\n",
        "input_queries = [preprocess_query(query) for query in <put desired query dataset here>]\n",
        "input_labels = <put desired target here> #(\"labels\" for JOB-light cardinality and \"costs\" for JOB-light costs)\n",
        "input_table_sizes = <put equivalent table sizes here>\n",
        "\n",
        "# limiting the number of queries to be used in the model\n",
        "input_queries = input_queries#[:number of queries to be used]\n",
        "input_labels = input_labels#[:number of queries to be used]\n",
        "input_table_sizes = input_table_sizes#[:number of queries to be used]\n",
        "\n",
        "# maximum query length before truncation, in number of tokens\n",
        "max_length = 256\n",
        "\n",
        "# tokenize dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/bert_uncased_L-4_H-256_A-4', use_fast=True)\n",
        "encoded_corpus = tokenizer(text=input_queries,\n",
        "                            add_special_tokens=True,\n",
        "                            padding='max_length',\n",
        "                            truncation='longest_first',\n",
        "                            max_length=max_length,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt')\n",
        "input_ids = encoded_corpus['input_ids']\n",
        "attention_mask = encoded_corpus['attention_mask']\n",
        "\n",
        "size = len(input_queries)\n",
        "\n",
        "# Create segment embeddings\n",
        "segments = [[0] * max_length]\n",
        "# Initialize with zeroes\n",
        "for i in range(size-1):\n",
        "  segments += [[0] * max_length]\n",
        "# For every query\n",
        "for i, query in enumerate(input_queries):\n",
        "    broken_query = query.replace(\">=\", \" >= \")\n",
        "    broken_query = broken_query.replace(\"<=\", \" <= \")\n",
        "    broken_query = broken_query.replace(\"!=\", \" != \")\n",
        "    broken_query = broken_query.replace(\">\", \" > \")\n",
        "    broken_query = broken_query.replace(\"<\", \" < \")\n",
        "    broken_query = broken_query.replace(\"=\", \" = \")\n",
        "    broken_query = broken_query.replace(\",\", \" , \")\n",
        "    j = 1\n",
        "    state = 0\n",
        "    # Label each token based on its basic syntactic purpose\n",
        "    for word in broken_query.split(\" \"):\n",
        "        # First for keywords\n",
        "        if word == 'SELECT':\n",
        "          state = 0\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 0\n",
        "            j += 1\n",
        "        elif word == 'FROM':\n",
        "          state = 1\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 1\n",
        "            j += 1\n",
        "        elif word == 'WHERE':\n",
        "          state = 2\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 2\n",
        "            j += 1\n",
        "        elif word == 'COUNT(*)':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 3\n",
        "            j += 1\n",
        "        elif word == 'AND':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 4\n",
        "            j += 1\n",
        "        elif word == 'UNION':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 5\n",
        "            j += 1\n",
        "        elif word == 'IN':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 6\n",
        "            j += 1\n",
        "        elif word == '=':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 7\n",
        "            j += 1\n",
        "        elif word == '>=':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 8\n",
        "            j += 1\n",
        "        elif word == '<=':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 9\n",
        "            j += 1\n",
        "        elif word == '>':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 10\n",
        "            j += 1\n",
        "        elif word == '<':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 11\n",
        "            j += 1\n",
        "        elif word == ',':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 12\n",
        "            j += 1\n",
        "        # Then for other terms\n",
        "        else:\n",
        "          # Return columns and tables\n",
        "          if state < 2:\n",
        "            for k in range(len(tokenizer.tokenize(word))):\n",
        "              segments[i][j] = 13 + state\n",
        "              j += 1\n",
        "          else:\n",
        "            # Column in expression\n",
        "            if \".\" in word:\n",
        "              for k in range(len(tokenizer.tokenize(word))):\n",
        "                segments[i][j] = 15\n",
        "                j += 1\n",
        "            # Value\n",
        "            else:\n",
        "              for k in range(len(tokenizer.tokenize(word))):\n",
        "                segments[i][j] = 16\n",
        "                j += 1\n",
        "    segments[i][j] = segments[i][j-1]\n",
        "\n",
        "# function changing embeddings into binary so that they are acceptable by BERT\n",
        "def change_to_binary(lst):\n",
        "    binary_lst = []\n",
        "    for i in range(len(lst)):\n",
        "        if i == 0:\n",
        "            binary_lst.append(0)\n",
        "        elif lst[i] != lst[i-1]:\n",
        "            binary_lst.append(1-binary_lst[i-1])\n",
        "        else:\n",
        "            binary_lst.append(binary_lst[i-1])\n",
        "    return binary_lst\n",
        "\n",
        "segments = [change_to_binary(segment) for segment in segments]\n",
        "\n",
        "# spliting data into train, validation and test sets\n",
        "\n",
        "input_ids_test = input_ids[int(size*0.8):]\n",
        "input_ids_val = input_ids[int(size*0.6):int(size*0.8)]\n",
        "input_ids_train = input_ids[:int(size*0.6)]\n",
        "\n",
        "attention_mask_test = attention_mask[int(size*0.8):]\n",
        "attention_mask_val = attention_mask[int(size*0.6):int(size*0.8)]\n",
        "attention_mask_train = attention_mask[:int(size*0.6)]\n",
        "\n",
        "segments_test = segments[int(size*0.8):]\n",
        "segments_val = segments[int(size*0.6):int(size*0.8)]\n",
        "segments_train = segments[:int(size*0.6)]\n",
        "\n",
        "labels_test = input_labels[int(size*0.8):]\n",
        "labels_val = input_labels[int(size*0.6):int(size*0.8)]\n",
        "labels_train = input_labels[:int(size*0.6)]\n",
        "\n",
        "table_sizes_test = input_table_sizes[int(size*0.8):]\n",
        "table_sizes_val = input_table_sizes[int(size*0.6):int(size*0.8)]\n",
        "table_sizes_train = input_table_sizes[:int(size*0.6)]\n",
        "\n",
        "# standardize all data, based on the statistics of the train set\n",
        "class Standardizer:\n",
        "    def __init__(self):\n",
        "        self.means = 0\n",
        "        self.stds = 0\n",
        "\n",
        "    def standardize_simple(self, train, validation, test):\n",
        "        train_stand = torch.tensor(train, dtype=torch.float)\n",
        "        validation_stand = torch.tensor(validation, dtype=torch.float)\n",
        "        test_stand = torch.tensor(test, dtype=torch.float)\n",
        "\n",
        "        stds = train_stand.std(dim=0, keepdim=True)\n",
        "        train_stand = train_stand / stds\n",
        "        validation_stand = validation_stand / stds\n",
        "        test_stand = test_stand / stds\n",
        "\n",
        "        train_stand = train_stand.detach().cpu().tolist()\n",
        "        validation_stand = validation_stand.detach().cpu().tolist()\n",
        "        test_stand = test_stand.detach().cpu().tolist()\n",
        "        return train_stand, validation_stand, test_stand\n",
        "\n",
        "    def standardize(self, train, validation, test):\n",
        "        train_stand = torch.tensor(train)\n",
        "        validation_stand = torch.tensor(validation)\n",
        "        test_stand = torch.tensor(test)\n",
        "\n",
        "        self.stds = train_stand.std(dim=0, keepdim=True)\n",
        "        train_stand = train_stand / self.stds\n",
        "        validation_stand = validation_stand / self.stds\n",
        "        test_stand = test_stand / self.stds\n",
        "\n",
        "        train_stand = train_stand.detach().cpu().tolist()\n",
        "        validation_stand = validation_stand.detach().cpu().tolist()\n",
        "        test_stand = test_stand.detach().cpu().tolist()\n",
        "        return train_stand, validation_stand, test_stand\n",
        "\n",
        "    def destandardize(self, data):\n",
        "        data_destand = torch.tensor(data)\n",
        "        data_destand = data_destand*self.stds\n",
        "        data_destand = data_destand.detach().cpu().tolist()\n",
        "        return data_destand\n",
        "\n",
        "standardizer = Standardizer()\n",
        "labels_train, labels_val, labels_test = standardizer.standardize(labels_train, labels_val, labels_test)\n",
        "table_sizes_train, table_sizes_val, table_sizes_test = standardizer.standardize_simple(table_sizes_train, table_sizes_val, table_sizes_test)\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# make dataloaders\n",
        "batch_size = 64\n",
        "\n",
        "def make_dataloader(inputs, masks, segments, labels, table_sizes, batch_size):\n",
        "    labels_tens = torch.tensor(labels)\n",
        "    segments_tens = torch.tensor(segments)\n",
        "    table_sizes_tens = torch.tensor(table_sizes, dtype=torch.float)\n",
        "    dataset = TensorDataset(inputs, masks, segments_tens, labels_tens, table_sizes_tens)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    return dataloader\n",
        "\n",
        "train_dataloader = make_dataloader(input_ids_train, attention_mask_train, segments_train, labels_train, table_sizes_train, batch_size)\n",
        "valid_dataloader = make_dataloader(input_ids_val, attention_mask_val, segments_val, labels_val, table_sizes_val, batch_size)\n",
        "test_dataloader = make_dataloader(input_ids_test, attention_mask_test, segments_test, labels_test, table_sizes_test, batch_size)\n",
        "\n",
        "labels_train = standardizer.destandardize(labels_train)\n",
        "labels_val = standardizer.destandardize(labels_val)\n",
        "labels_test = standardizer.destandardize(labels_test)\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers import BertForSequenceClassification, BertConfig, BertModel\n",
        "\n",
        "# load bert model within our defined network\n",
        "config = BertConfig.from_pretrained('google/bert_uncased_L-4_H-256_A-4', num_labels=2, hidden_dropout_prob=0.3,\n",
        "                                    attention_probs_dropout_prob=0.3, output_attentions = False, output_hidden_states = True)\n",
        "\n",
        "class BertRegressor(nn.Module):\n",
        "\n",
        "    def __init__(self, drop_rate=0.3, config=config):\n",
        "\n",
        "        super(BertRegressor, self).__init__()\n",
        "        D_in, D_out = 256, 1\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('google/bert_uncased_L-4_H-256_A-4', config=config)\n",
        "        self.drop = nn.Dropout(drop_rate)\n",
        "        self.linear = nn.Linear(D_in+4, D_out)  #+4 because we are adding table sizes\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, input_ids, attention_masks, segments, table_sizes):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_masks, token_type_ids=segments)\n",
        "        hidden_states = outputs[2]\n",
        "        token_vecs = hidden_states[-2][:]\n",
        "        token_vecs = token_vecs.permute(1,0,2)\n",
        "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "        outputs = self.drop(sentence_embedding)\n",
        "        embedding_with_sizes = torch.cat(tensors=[outputs, table_sizes], dim=1)\n",
        "        outputs = self.linear(embedding_with_sizes)\n",
        "        outputs = self.relu(outputs)\n",
        "        outputs = torch.squeeze(outputs, 1)\n",
        "        return outputs\n",
        "\n",
        "model = BertRegressor()\n",
        "\n",
        "# connect to gpu\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# define optimizer, with scheduler for adapting learning rate\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 50 # define maximum number of epochs\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=steps)\n",
        "\n",
        "# define loss function\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# imports for statistics\n",
        "from sklearn.metrics import r2_score\n",
        "import statistics\n",
        "\n",
        "# define statistics\n",
        "list_r2=[]\n",
        "list_r2_train=[]\n",
        "list_qerror=[]\n",
        "list_qerror_train=[]\n",
        "list_sample_size=[]\n",
        "\n",
        "# function to predict using model\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    output = []\n",
        "    for batch in dataloader:\n",
        "        batch_inputs, batch_masks, batch_segments, batch_labels, batch_table_sizes = tuple(b.to(device) for b in batch)\n",
        "        with torch.no_grad():\n",
        "            output += model(batch_inputs, batch_masks, batch_segments, batch_table_sizes).view(1,-1).tolist()[0]\n",
        "    return output\n",
        "\n",
        "# function to train model\n",
        "def train(model, optimizer, scheduler, loss_function, epochs,\n",
        "          train_dataloader, valid_dataloader, device, clip_value=2):\n",
        "    best_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(\"-----\")\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        y_train = []\n",
        "        print(\"Training:\")\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            if step == 0:\n",
        "              print(\"1/\"+str(len(train_dataloader)), end='')\n",
        "            else:\n",
        "              print('\\b'*(len(str(step)) + len(str(len(train_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(train_dataloader)), end='')\n",
        "\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_segment, b_labels, b_table_sizes = batch\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            predictions = model(b_input_ids, b_input_mask, b_segment, b_table_sizes)\n",
        "            y_train += predictions.detach().cpu().tolist()\n",
        "            loss = loss_function(predictions, b_labels)\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "            # Update model's weights based on the gradients calculated during backpropagation\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+12))\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_dataloader)\n",
        "        print(f\"Training Loss: {avg_train_loss}\")\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        model.eval()\n",
        "        eval_loss = 0.0\n",
        "\n",
        "        y_val = []\n",
        "        with torch.no_grad():\n",
        "            print(\"Validating:\")\n",
        "            for step, batch in enumerate(valid_dataloader):\n",
        "                if step == 0:\n",
        "                  print(\"1/\"+str(len(valid_dataloader)), end='')\n",
        "                else:\n",
        "                  print('\\b'*(len(str(step)) + len(str(len(valid_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(valid_dataloader)), end='')\n",
        "\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                b_input_ids, b_input_mask, b_segment, b_labels, b_table_sizes = batch\n",
        "\n",
        "                eval_output =  model(b_input_ids, b_input_mask, b_segment, b_table_sizes)\n",
        "                y_val += eval_output.detach().cpu().tolist()\n",
        "                loss = loss_function(eval_output, b_labels)\n",
        "\n",
        "                eval_loss += loss\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+14))\n",
        "\n",
        "        avg_eval_loss = eval_loss / len(valid_dataloader)\n",
        "        print(f\"Validation Loss: {avg_eval_loss}\")\n",
        "\n",
        "        # calculate extra statistics for visualisation\n",
        "        y_train = standardizer.destandardize(y_train)\n",
        "        y_val = standardizer.destandardize(y_val)\n",
        "\n",
        "        r2_train = r2_score(labels_train, y_train)\n",
        "        print(\"R² Train: \" + str(r2_train))\n",
        "        r2 = r2_score(labels_val, y_val)\n",
        "        print(\"R² Validation: \" + str(r2))\n",
        "\n",
        "        qerror_train = statistics.mean([max(labels_train[i], y_train[i])/min(labels_train[i], y_train[i] if y_train[i] > 0 else 1e-10) for i in range(len(labels_train))])\n",
        "        print(\"Q-error Train: \" + str(qerror_train))\n",
        "        qerror = statistics.mean([max(labels_val[i], y_val[i])/min(labels_val[i], y_val[i] if y_val[i] > 0 else 1e-10) for i in range(len(labels_val))])\n",
        "        print(\"Q-error Validation: \" + str(qerror))\n",
        "        print()\n",
        "\n",
        "        list_r2.append(r2)\n",
        "        list_r2_train.append(r2_train)\n",
        "        list_qerror.append(qerror)\n",
        "        list_qerror_train.append(qerror_train)\n",
        "        list_sample_size.append((epoch * 0.1 + 0.1))\n",
        "\n",
        "        # Early stopping based on validation loss\n",
        "        if avg_eval_loss < best_loss:\n",
        "            best_loss = avg_eval_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement == 5:\n",
        "                print(\"Validation loss has not improved in 5 epochs, stopping early.\")\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train model\n",
        "model = train(model, optimizer, scheduler, loss_function, epochs,\n",
        "              train_dataloader, valid_dataloader, device, clip_value=2)\n",
        "# Testing\n",
        "\n",
        "# define r2_score function\n",
        "def r2_score(outputs, labels):\n",
        "    labels_mean = statistics.mean(labels)\n",
        "    ss_tot = sum([(x-labels_mean)**2 for x in labels])\n",
        "    ss_res = sum([(x-y)**2 for x,y in zip(labels, outputs)])\n",
        "    r2 = (1 - (ss_res / (ss_tot if ss_tot != 0 else 1e-10)))\n",
        "    return r2\n",
        "\n",
        "# Measure performance on test set\n",
        "y_test = labels_test\n",
        "y_pred = predict(model, test_dataloader, device)\n",
        "y_pred = standardizer.destandardize(y_pred)\n",
        "\n",
        "# more imports for metrics to be used on test set\n",
        "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "import pandas as pd\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"MAE Test: \", mae)\n",
        "mdae = median_absolute_error(y_test, y_pred)\n",
        "print(\"MDAE Test: \", mdae)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"MSE Test: \", mse)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "print(\"MAPE Test: \", mape)\n",
        "mdape = ((pd.Series(y_test) - pd.Series(y_pred)) / pd.Series(y_test)).abs().median()\n",
        "print(\"MDAPE Test: \", mdape)\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "print(\"R² Test: \", r_squared)\n",
        "qerror = statistics.mean([max(y_test[i], y_pred[i] if y_pred[i] != 0 else train_mean)/min(y_test[i], y_pred[i] if y_pred[i] > 0 else train_mean) for i in range(len(y_test))])\n",
        "print(\"Q-error Test: \", qerror)"
      ],
      "metadata": {
        "id": "BtuSdaXyENJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#L − 8H − 256A − 4"
      ],
      "metadata": {
        "id": "P-PSHz7TzC23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports\n",
        "import re\n",
        "import statistics\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "# function transforming numerical values in our dataset into tokens representing ranges\n",
        "def num_to_token(num):\n",
        "  if num < 1850:\n",
        "    return str(num)\n",
        "  if num < 2000:\n",
        "    return \"year1\"\n",
        "  elif num < 2010:\n",
        "    return \"year2\"\n",
        "  else:\n",
        "    return \"year3\"\n",
        "\n",
        "# function performing basic preprocessing\n",
        "def preprocess_query(query):\n",
        "  return re.sub(r\"(\\d+)\", lambda x: num_to_token(int(x.group(0))), query)\n",
        "\n",
        "# obtaining input queries and labels (cardinality or cost)\n",
        "input_queries = [preprocess_query(query) for query in <put desired query dataset here>]\n",
        "input_labels = <put desired target here> #(\"labels\" for JOB-light cardinality and \"costs\" for JOB-light costs)\n",
        "\n",
        "# limiting the number of queries to be used in the model\n",
        "input_queries = input_queries#[:number of queries to be used]\n",
        "input_labels = input_labels#[:number of queries to be used]\n",
        "\n",
        "# maximum query length before truncation, in number of tokens\n",
        "max_length = 256\n",
        "\n",
        "# tokenize dataset\n",
        "size = len(input_queries)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/bert_uncased_L-8_H-256_A-4', use_fast=True)\n",
        "encoded_corpus = tokenizer(text=input_queries[:int(size*0.8)],\n",
        "                            add_special_tokens=True,\n",
        "                            padding='max_length',\n",
        "                            truncation='longest_first',\n",
        "                            max_length=max_length,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt')\n",
        "input_ids = encoded_corpus['input_ids']\n",
        "attention_mask = encoded_corpus['attention_mask']\n",
        "\n",
        "encoded_corpus_test = tokenizer(text=input_queries[int(size*0.8):],\n",
        "                            add_special_tokens=True,\n",
        "                            padding='max_length',\n",
        "                            truncation='longest_first',\n",
        "                            max_length=max_length,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt')\n",
        "input_ids_test = encoded_corpus_test['input_ids']\n",
        "attention_mask_test = encoded_corpus_test['attention_mask']\n",
        "\n",
        "# split train, validation and test sets\n",
        "input_ids_val = input_ids[int(size*0.6):int(size*0.8)]\n",
        "input_ids_train = input_ids[:int(size*0.6)]\n",
        "\n",
        "attention_mask_val = attention_mask[int(size*0.6):int(size*0.8)]\n",
        "attention_mask_train = attention_mask[:int(size*0.6)]\n",
        "\n",
        "labels_test = input_labels[int(size*0.8):]\n",
        "labels_val = input_labels[int(size*0.6):int(size*0.8)]\n",
        "labels_train = input_labels[:int(size*0.6)]\n",
        "\n",
        "# calculate the mean of the target in the train data\n",
        "train_mean = statistics.mean(labels_train)\n",
        "\n",
        "# standardize all data, based on the statistics of the train set\n",
        "class Standardizer:\n",
        "    def __init__(self):\n",
        "        self.means = 0\n",
        "        self.stds = 0\n",
        "\n",
        "    def standardize(self, train, validation, test):\n",
        "        train_stand = torch.tensor(train)\n",
        "        validation_stand = torch.tensor(validation)\n",
        "        test_stand = torch.tensor(test)\n",
        "\n",
        "        self.stds = train_stand.std(dim=0, keepdim=True)\n",
        "        train_stand = train_stand / self.stds\n",
        "        validation_stand = validation_stand / self.stds\n",
        "        test_stand = test_stand / self.stds\n",
        "\n",
        "        train_stand = train_stand.detach().cpu().tolist()\n",
        "        validation_stand = validation_stand.detach().cpu().tolist()\n",
        "        test_stand = test_stand.detach().cpu().tolist()\n",
        "        return train_stand, validation_stand, test_stand\n",
        "\n",
        "    def destandardize(self, data):\n",
        "        data_destand = torch.tensor(data)\n",
        "        data_destand = data_destand*self.stds\n",
        "        data_destand = data_destand.detach().cpu().tolist()\n",
        "        return data_destand\n",
        "\n",
        "standardizer = Standardizer()\n",
        "labels_train, labels_val, labels_test = standardizer.standardize(labels_train, labels_val, labels_test)\n",
        "\n",
        "# make dataloaders\n",
        "batch_size = 64\n",
        "\n",
        "def make_dataloader(inputs, masks, labels, batch_size):\n",
        "    labels_tens = torch.tensor(labels)\n",
        "    dataset = TensorDataset(inputs, masks, labels_tens)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    return dataloader\n",
        "\n",
        "train_dataloader = make_dataloader(input_ids_train, attention_mask_train, labels_train, batch_size)\n",
        "valid_dataloader = make_dataloader(input_ids_val, attention_mask_val, labels_val, batch_size)\n",
        "test_dataloader = make_dataloader(input_ids_test, attention_mask_test, labels_test, batch_size)\n",
        "\n",
        "\n",
        "# destandardize labels after passing them to the dataloaders,\n",
        "# we want them standardized within the model and unstandardized outside\n",
        "labels_train = standardizer.destandardize(labels_train)\n",
        "labels_val = standardizer.destandardize(labels_val)\n",
        "labels_test = standardizer.destandardize(labels_test)\n",
        "\n",
        "# load BERT model and define our head\n",
        "config = BertConfig.from_pretrained('google/bert_uncased_L-4_H-256_A-4', num_labels=2, hidden_dropout_prob=0.3,\n",
        "                                    attention_probs_dropout_prob=0.3, output_attentions = False, output_hidden_states = True)\n",
        "\n",
        "class BertRegressor(nn.Module):\n",
        "    def __init__(self, drop_rate=0.3, config=config):\n",
        "\n",
        "        super(BertRegressor, self).__init__()\n",
        "        D_in, D_out = 256, 1\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('google/bert_uncased_L-4_H-256_A-4', config=config)\n",
        "        self.drop = nn.Dropout(drop_rate)\n",
        "        self.linear = nn.Linear(D_in, D_out)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, input_ids, attention_masks):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_masks)\n",
        "        hidden_states = outputs[2]\n",
        "        token_vecs = hidden_states[-2][:]\n",
        "        token_vecs = token_vecs.permute(1,0,2)\n",
        "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "        outputs = self.drop(sentence_embedding)\n",
        "        outputs = self.linear(outputs)\n",
        "        outputs = self.relu(outputs)\n",
        "        outputs = torch.squeeze(outputs, 1)\n",
        "        return outputs\n",
        "\n",
        "model = BertRegressor()\n",
        "\n",
        "# connect to GPU, if available\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# define optimizer, with scheduler for adapting learning rate\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 50 # define maximum number of epochs\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=steps)\n",
        "\n",
        "# define loss function\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# imports for statistics\n",
        "from sklearn.metrics import r2_score\n",
        "import statistics\n",
        "\n",
        "# define statistics\n",
        "list_r2=[]\n",
        "list_r2_train=[]\n",
        "list_qerror=[]\n",
        "list_qerror_train=[]\n",
        "list_sample_size=[]\n",
        "\n",
        "# function to predict using model\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    output = []\n",
        "    for batch in dataloader:\n",
        "        batch_inputs, batch_masks, batch_labels = tuple(b.to(device) for b in batch)\n",
        "        with torch.no_grad():\n",
        "            output += model(batch_inputs, batch_masks).view(1,-1).tolist()[0]\n",
        "    return output\n",
        "\n",
        "# function to train model\n",
        "def train(model, optimizer, scheduler, loss_function, epochs,\n",
        "          train_dataloader, valid_dataloader, device, clip_value=2):\n",
        "    best_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # print progress\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(\"-----\")\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        y_train = []\n",
        "        print(\"Training:\")\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            if step == 0:\n",
        "              print(\"1/\"+str(len(train_dataloader)), end='')\n",
        "            else:\n",
        "              print('\\b'*(len(str(step)) + len(str(len(train_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(train_dataloader)), end='')\n",
        "\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            predictions = model(b_input_ids, b_input_mask)\n",
        "            y_train += predictions.detach().cpu().tolist()\n",
        "            loss = loss_function(predictions, b_labels)\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "            # Update model's weights based on the gradients calculated during backpropagation\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+12))\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_dataloader)\n",
        "        print(f\"Training Loss: {avg_train_loss}\")\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        model.eval()\n",
        "        eval_loss = 0.0\n",
        "\n",
        "        y_val = []\n",
        "        with torch.no_grad():\n",
        "            print(\"Validating:\")\n",
        "            for step, batch in enumerate(valid_dataloader):\n",
        "                if step == 0:\n",
        "                  print(\"1/\"+str(len(valid_dataloader)), end='')\n",
        "                else:\n",
        "                  print('\\b'*(len(str(step)) + len(str(len(valid_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(valid_dataloader)), end='')\n",
        "\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "                eval_output =  model(b_input_ids, b_input_mask)\n",
        "                y_val += eval_output.detach().cpu().tolist()\n",
        "                loss = loss_function(eval_output, b_labels)\n",
        "\n",
        "                eval_loss += loss\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+14))\n",
        "\n",
        "        avg_eval_loss = eval_loss / len(valid_dataloader)\n",
        "        print(f\"Validation Loss: {avg_eval_loss}\")\n",
        "\n",
        "        # calculate extra statistics for the output\n",
        "        y_train = standardizer.destandardize(y_train)\n",
        "        y_val = standardizer.destandardize(y_val)\n",
        "\n",
        "        r2_train = r2_score(labels_train, y_train)\n",
        "        print(\"R² Train: \" + str(r2_train))\n",
        "        r2 = r2_score(labels_val, y_val)\n",
        "        print(\"R² Validation: \" + str(r2))\n",
        "\n",
        "        qerror_train = statistics.mean([max(labels_train[i], y_train[i])/min(labels_train[i], y_train[i] if y_train[i] > 0 else train_mean) for i in range(len(labels_train))])\n",
        "        print(\"Q-error Train: \" + str(qerror_train))\n",
        "        qerror = statistics.mean([max(labels_val[i], y_val[i])/min(labels_val[i], y_val[i] if y_val[i] > 0 else train_mean) for i in range(len(labels_val))])\n",
        "        print(\"Q-error Validation: \" + str(qerror))\n",
        "        print()\n",
        "\n",
        "        list_r2.append(r2)\n",
        "        list_r2_train.append(r2_train)\n",
        "        list_qerror.append(qerror)\n",
        "        list_qerror_train.append(qerror_train)\n",
        "        list_sample_size.append((epoch * 0.1 + 0.1))\n",
        "\n",
        "        # Early stopping based on validation loss\n",
        "        if avg_eval_loss < best_loss:\n",
        "            best_loss = avg_eval_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement == 5:\n",
        "                print(\"Validation loss has not improved in 5 epochs, stopping early.\")\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train model\n",
        "model = train(model, optimizer, scheduler, loss_function, epochs,\n",
        "              train_dataloader, valid_dataloader, device, clip_value=2)\n",
        "# Testing\n",
        "\n",
        "# define r2_score function\n",
        "def r2_score(outputs, labels):\n",
        "    labels_mean = statistics.mean(labels)\n",
        "    ss_tot = sum([(x-labels_mean)**2 for x in labels])\n",
        "    ss_res = sum([(x-y)**2 for x,y in zip(labels, outputs)])\n",
        "    r2 = (1 - (ss_res / (ss_tot if ss_tot != 0 else 1e-10)))\n",
        "    return r2\n",
        "\n",
        "# Measure performance on test set\n",
        "y_test = labels_test\n",
        "y_pred = predict(model, test_dataloader, device)\n",
        "y_pred = standardizer.destandardize(y_pred)\n",
        "\n",
        "# more imports for metrics to be used on test set\n",
        "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "import pandas as pd\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"MAE Test: \", mae)\n",
        "mdae = median_absolute_error(y_test, y_pred)\n",
        "print(\"MDAE Test: \", mdae)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"MSE Test: \", mse)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "print(\"MAPE Test: \", mape)\n",
        "mdape = ((pd.Series(y_test) - pd.Series(y_pred)) / pd.Series(y_test)).abs().median()\n",
        "print(\"MDAPE Test: \", mdape)\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "print(\"R² Test: \", r_squared)\n",
        "qerror = statistics.mean([max(y_test[i], y_pred[i] if y_pred[i] != 0 else train_mean)/min(y_test[i], y_pred[i] if y_pred[i] > 0 else train_mean) for i in range(len(y_test))])\n",
        "print(\"Q-error Test: \", qerror)"
      ],
      "metadata": {
        "id": "jlA9yEdD4y-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SyntaxBERT"
      ],
      "metadata": {
        "id": "QJ-STRFny6_d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nkoJfsmIp2e"
      },
      "outputs": [],
      "source": [
        "!pip install fastdtw"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports\n",
        "import re\n",
        "import statistics\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from transformers import BertConfig, BertModel\n",
        "from fastdtw import fastdtw\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# function transforming numerical values in our dataset into tokens representing ranges\n",
        "def num_to_token(num):\n",
        "  if num < 1850:\n",
        "    return str(num)\n",
        "  if num < 2000:\n",
        "    return \"year1\"\n",
        "  elif num < 2010:\n",
        "    return \"year2\"\n",
        "  else:\n",
        "    return \"year3\"\n",
        "\n",
        "# function performing basic preprocessing\n",
        "def preprocess_query(query):\n",
        "  return re.sub(r\"(\\d+)\", lambda x: num_to_token(int(x.group(0))), query)\n",
        "\n",
        "# obtaining input queries and labels (cardinality or cost)\n",
        "input_queries = [preprocess_query(query) for query in <put desired query dataset here>]\n",
        "input_labels = <put desired target here> #(\"labels\" for JOB-light cardinality and \"costs\" for JOB-light costs)\n",
        "\n",
        "# limiting the number of queries to be used in the model\n",
        "input_queries = input_queries#[:number of queries to be used]\n",
        "input_labels = input_labels#[:number of queries to be used]\n",
        "\n",
        "# maximum query length before truncation, in number of tokens\n",
        "max_length = 256\n",
        "\n",
        "# tokenize dataset\n",
        "size = len(input_queries)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/bert_uncased_L-4_H-256_A-4', use_fast=True)\n",
        "encoded_corpus = tokenizer(text=input_queries[:int(size*0.8)],\n",
        "                            add_special_tokens=True,\n",
        "                            padding='max_length',\n",
        "                            truncation='longest_first',\n",
        "                            max_length=max_length,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt')\n",
        "input_ids = encoded_corpus['input_ids']\n",
        "attention_mask = encoded_corpus['attention_mask']\n",
        "\n",
        "encoded_corpus_test = tokenizer(text=input_queries[int(size*0.8):],\n",
        "                            add_special_tokens=True,\n",
        "                            padding='max_length',\n",
        "                            truncation='longest_first',\n",
        "                            max_length=max_length,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt')\n",
        "input_ids_test = encoded_corpus_test['input_ids']\n",
        "attention_mask_test = encoded_corpus_test['attention_mask']\n",
        "\n",
        "# split train, validation and test sets\n",
        "input_ids_val = input_ids[int(size*0.6):int(size*0.8)]\n",
        "input_ids_train = input_ids[:int(size*0.6)]\n",
        "\n",
        "attention_mask_val = attention_mask[int(size*0.6):int(size*0.8)]\n",
        "attention_mask_train = attention_mask[:int(size*0.6)]\n",
        "\n",
        "labels_test = input_labels[int(size*0.8):]\n",
        "labels_val = input_labels[int(size*0.6):int(size*0.8)]\n",
        "labels_train = input_labels[:int(size*0.6)]\n",
        "\n",
        "# calculate the mean of the target in the train data\n",
        "train_mean = statistics.mean(labels_train)\n",
        "\n",
        "# Create segment embeddings\n",
        "segments = [[0] * max_length]\n",
        "# Initialize with zeroes\n",
        "for i in range(size-1):\n",
        "    segments += [[0] * max_length]\n",
        "# For every query\n",
        "for i, query in enumerate(input_queries):\n",
        "    broken_query = query.replace(\">=\", \" >= \")\n",
        "    broken_query = broken_query.replace(\"<=\", \" <= \")\n",
        "    broken_query = broken_query.replace(\"!=\", \" != \")\n",
        "    broken_query = broken_query.replace(\">\", \" > \")\n",
        "    broken_query = broken_query.replace(\"<\", \" < \")\n",
        "    broken_query = broken_query.replace(\"=\", \" = \")\n",
        "    broken_query = broken_query.replace(\",\", \" , \")\n",
        "    j = 1\n",
        "    state = 0\n",
        "    # Label each token based on its basic syntactic purpose\n",
        "    for word in broken_query.split(\" \"):\n",
        "        # First for keywords\n",
        "        if word == 'SELECT':\n",
        "          state = 0\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 0\n",
        "            j += 1\n",
        "        elif word == 'FROM':\n",
        "          state = 1\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 1\n",
        "            j += 1\n",
        "        elif word == 'WHERE':\n",
        "          state = 2\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 2\n",
        "            j += 1\n",
        "        elif word == 'COUNT(*)':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 3\n",
        "            j += 1\n",
        "        elif word == 'AND':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 4\n",
        "            j += 1\n",
        "        elif word == 'UNION':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 5\n",
        "            j += 1\n",
        "        elif word == 'IN':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 6\n",
        "            j += 1\n",
        "        elif word == '=':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 7\n",
        "            j += 1\n",
        "        elif word == '>=':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 8\n",
        "            j += 1\n",
        "        elif word == '<=':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 9\n",
        "            j += 1\n",
        "        elif word == '>':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 10\n",
        "            j += 1\n",
        "        elif word == '<':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 11\n",
        "            j += 1\n",
        "        elif word == ',':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 12\n",
        "            j += 1\n",
        "        # Then for other terms\n",
        "        else:\n",
        "          # Return columns and tables\n",
        "          if state < 2:\n",
        "            for k in range(len(tokenizer.tokenize(word))):\n",
        "              segments[i][j] = 13 + state\n",
        "              j += 1\n",
        "          else:\n",
        "            # Column in expression\n",
        "            if \".\" in word:\n",
        "              for k in range(len(tokenizer.tokenize(word))):\n",
        "                segments[i][j] = 15\n",
        "                j += 1\n",
        "            # Value\n",
        "            else:\n",
        "              for k in range(len(tokenizer.tokenize(word))):\n",
        "                segments[i][j] = 16\n",
        "                j += 1\n",
        "    segments[i][j] = segments[i][j-1]\n",
        "    segments[i] = segments[i][:j+1]\n",
        "\n",
        "# split the segments into a train, a validation and a test set\n",
        "segments_test = segments[int(size*0.8):]\n",
        "segments_val = segments[int(size*0.6):int(size*0.8)]\n",
        "segments_train = segments[:int(size*0.6)]\n",
        "\n",
        "n = len(segments_train)\n",
        "# Calculate DTW distances in the train set and create a distance matrix\n",
        "distance_matrix = np.zeros((n, n))\n",
        "for i in range(n):\n",
        "    for j in range(i, n):\n",
        "        distance, _ = fastdtw(segments_train[i], segments_train[j])\n",
        "        distance_matrix[i][j] = distance\n",
        "        distance_matrix[j][i] = distance\n",
        "\n",
        "# Perform K-Means clustering\n",
        "num_clusters = 5  # Adjust the number of clusters as needed\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(distance_matrix)\n",
        "\n",
        "# Perform PCA for dimensionality reduction\n",
        "pca = PCA(n_components=2)\n",
        "reduced_data = pca.fit_transform(distance_matrix)\n",
        "\n",
        "# Visualize the cluster assignments as points on a 2D plane\n",
        "plt.figure(figsize=(8, 6))\n",
        "colors = ['r', 'g', 'b', 'm', 'y']  # Assign colors to clusters\n",
        "for i in range(num_clusters):\n",
        "    cluster_indices = np.where(kmeans.labels_ == i)[0]\n",
        "    plt.scatter(reduced_data[cluster_indices, 0], reduced_data[cluster_indices, 1], label=f'Cluster {i}', c=colors[i], s=50)\n",
        "\n",
        "plt.xlabel('PCA Dimension 1')\n",
        "plt.ylabel('PCA Dimension 2')\n",
        "plt.title('K-Means Clustering Visualization of Time Series Data')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Extract points of each cluster\n",
        "cluster_points = [[] for _ in range(num_clusters)]\n",
        "for i in range(len(segments_train)):\n",
        "    cluster_index = kmeans.labels_[i]\n",
        "    cluster_points[cluster_index].append(input_labels[i])\n",
        "\n",
        "# Average the points of each cluster\n",
        "centroid_labels = []\n",
        "for points in cluster_points:\n",
        "    cur_sum = 0\n",
        "    for point in points:\n",
        "        cur_sum += point\n",
        "    average = cur_sum/len(points)\n",
        "    centroid_labels.append(average)\n",
        "\n",
        "# Get the cluster centroids\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "# Match the train set to a clusters\n",
        "predicted_from_clust_train = []\n",
        "for i in range(len(segments_train)):\n",
        "    centr_pos = np.argmin([fastdtw(segments_train[i], centroid)[0] for centroid in centroids])\n",
        "    matching_centr_label = centroid_labels[centr_pos]\n",
        "    predicted_from_clust_train.append(matching_centr_label)\n",
        "\n",
        "# Match the val set to a clusters\n",
        "predicted_from_clust_val = []\n",
        "for i in range(len(segments_val)):\n",
        "    centr_pos = np.argmin([fastdtw(segments_val[i], centroid)[0] for centroid in centroids])\n",
        "    matching_centr_label = centroid_labels[centr_pos]\n",
        "    predicted_from_clust_val.append(matching_centr_label)\n",
        "\n",
        "# Match the test set to a clusters\n",
        "predicted_from_clust_test = []\n",
        "for i in range(len(segments_test)):\n",
        "    centr_pos = np.argmin([fastdtw(segments_test[i], centroid)[0] for centroid in centroids])\n",
        "    matching_centr_label = centroid_labels[centr_pos]\n",
        "    predicted_from_clust_test.append(matching_centr_label)\n",
        "\n",
        "# standardize all data, based on the statistics of the train set\n",
        "class Standardizer:\n",
        "    def __init__(self):\n",
        "        self.means = 0\n",
        "        self.stds = 0\n",
        "\n",
        "    def standardize(self, train, validation, test, clust_train, clust_val, clust_test):\n",
        "        train_stand = torch.tensor(train)\n",
        "        validation_stand = torch.tensor(validation)\n",
        "        test_stand = torch.tensor(test)\n",
        "        clust_train_stand = torch.tensor(clust_train)\n",
        "        clust_val_stand = torch.tensor(clust_val)\n",
        "        clust_test_stand = torch.tensor(clust_test)\n",
        "\n",
        "        self.stds = train_stand.std(dim=0, keepdim=True)\n",
        "        train_stand = train_stand / self.stds\n",
        "        validation_stand = validation_stand / self.stds\n",
        "        test_stand = test_stand / self.stds\n",
        "        clust_train_stand = clust_train_stand / self.stds\n",
        "        clust_val_stand = clust_val_stand / self.stds\n",
        "        clust_test_stand = clust_test_stand / self.stds\n",
        "\n",
        "        train_stand = train_stand.detach().cpu().tolist()\n",
        "        validation_stand = validation_stand.detach().cpu().tolist()\n",
        "        test_stand = test_stand.detach().cpu().tolist()\n",
        "        clust_train_stand = clust_train_stand.detach().cpu().tolist()\n",
        "        clust_val_stand = clust_val_stand.detach().cpu().tolist()\n",
        "        clust_test_stand = clust_test_stand.detach().cpu().tolist()\n",
        "        return train_stand, validation_stand, test_stand, clust_train_stand, clust_val_stand, clust_test_stand\n",
        "\n",
        "    def destandardize(self, data):\n",
        "        data_destand = torch.tensor(data)\n",
        "        data_destand = data_destand*self.stds\n",
        "        data_destand = data_destand.detach().cpu().tolist()\n",
        "        return data_destand\n",
        "\n",
        "standardizer = Standardizer()\n",
        "labels_train, labels_val, labels_test, predicted_from_clust_train, predicted_from_clust_val, predicted_from_clust_test = standardizer.standardize(labels_train, labels_val, labels_test, predicted_from_clust_train, predicted_from_clust_val, predicted_from_clust_test)\n",
        "\n",
        "# make dataloaders\n",
        "batch_size = 64\n",
        "\n",
        "def make_dataloader(inputs, masks, labels, predicted_from_clust, batch_size):\n",
        "    labels_tens = torch.tensor(labels)\n",
        "    predicted_from_clust_tens = torch.tensor(predicted_from_clust)\n",
        "    dataset = TensorDataset(inputs, masks, labels_tens, predicted_from_clust_tens)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    return dataloader\n",
        "\n",
        "train_dataloader = make_dataloader(input_ids_train, attention_mask_train, labels_train, predicted_from_clust_train, batch_size)\n",
        "valid_dataloader = make_dataloader(input_ids_val, attention_mask_val, labels_val, predicted_from_clust_val, batch_size)\n",
        "test_dataloader = make_dataloader(input_ids_test, attention_mask_test, labels_test, predicted_from_clust_test, batch_size)\n",
        "\n",
        "# destandardize labels after passing them to the dataloaders,\n",
        "# we want them standardized within the model and unstandardized outside\n",
        "labels_train = standardizer.destandardize(labels_train)\n",
        "labels_val = standardizer.destandardize(labels_val)\n",
        "labels_test = standardizer.destandardize(labels_test)\n",
        "predicted_from_clust_train = standardizer.destandardize(predicted_from_clust_train)\n",
        "predicted_from_clust_val = standardizer.destandardize(predicted_from_clust_val)\n",
        "predicted_from_clust_test = standardizer.destandardize(predicted_from_clust_test)\n",
        "\n",
        "# load BERT model and define our head\n",
        "config = BertConfig.from_pretrained('google/bert_uncased_L-4_H-256_A-4', num_labels=2, hidden_dropout_prob=0.3,\n",
        "                                    attention_probs_dropout_prob=0.3, output_attentions = False, output_hidden_states = True)\n",
        "\n",
        "\n",
        "class BertRegressor(nn.Module):\n",
        "\n",
        "    def __init__(self, drop_rate=0.3, config=config):\n",
        "\n",
        "        super(BertRegressor, self).__init__()\n",
        "        D_in, D_out = 256, 1\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('google/bert_uncased_L-4_H-256_A-4', config=config)\n",
        "        self.drop = nn.Dropout(drop_rate)\n",
        "        self.linear = nn.Linear(D_in, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(2, D_out)\n",
        "        self.relu2 = nn.ReLU()\n",
        "    def forward(self, input_ids, attention_masks, predicted_from_clust):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_masks)\n",
        "        hidden_states = outputs[2]\n",
        "        token_vecs = hidden_states[-2][:]\n",
        "        token_vecs = token_vecs.permute(1,0,2)\n",
        "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "        outputs = self.drop(sentence_embedding)\n",
        "        outputs = self.linear(outputs)\n",
        "        outputs = self.relu(outputs)\n",
        "        outputs = torch.squeeze(outputs, 1)\n",
        "        outputs = torch.stack((outputs, predicted_from_clust),1)\n",
        "        outputs = self.linear2(outputs)\n",
        "        outputs = self.relu2(outputs)\n",
        "        outputs = torch.squeeze(outputs, 1)\n",
        "        return outputs\n",
        "\n",
        "model = BertRegressor()\n",
        "\n",
        "# connect to GPU, if available\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# define optimizer, with scheduler for adapting learning rate\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 50 # define maximum number of epochs\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=steps)\n",
        "\n",
        "# define loss function\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# imports for statistics\n",
        "from sklearn.metrics import r2_score\n",
        "import statistics\n",
        "\n",
        "# define statistics\n",
        "list_r2=[]\n",
        "list_r2_train=[]\n",
        "list_qerror=[]\n",
        "list_qerror_train=[]\n",
        "list_sample_size=[]\n",
        "\n",
        "# function to predict using model\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    output = []\n",
        "    for batch in dataloader:\n",
        "        batch_inputs, batch_masks, batch_labels, batch_clust = tuple(b.to(device) for b in batch)\n",
        "        with torch.no_grad():\n",
        "            output += model(batch_inputs, batch_masks, batch_clust).view(1,-1).tolist()[0]\n",
        "    return output\n",
        "\n",
        "# function to train model\n",
        "def train(model, optimizer, scheduler, loss_function, epochs,\n",
        "          train_dataloader, valid_dataloader, device, clip_value=2):\n",
        "    best_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # print progress\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(\"-----\")\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        y_train = []\n",
        "        print(\"Training:\")\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            if step == 0:\n",
        "              print(\"1/\"+str(len(train_dataloader)), end='')\n",
        "            else:\n",
        "              print('\\b'*(len(str(step)) + len(str(len(train_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(train_dataloader)), end='')\n",
        "\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels, b_clust = batch\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            predictions = model(b_input_ids, b_input_mask, b_clust)\n",
        "            y_train += predictions.detach().cpu().tolist()\n",
        "            loss = loss_function(predictions, b_labels)\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "            # Update model's weights based on the gradients calculated during backpropagation\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+12))\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_dataloader)\n",
        "        print(f\"Training Loss: {avg_train_loss}\")\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        model.eval()\n",
        "        eval_loss = 0.0\n",
        "\n",
        "        y_val = []\n",
        "        with torch.no_grad():\n",
        "            print(\"Validating:\")\n",
        "            for step, batch in enumerate(valid_dataloader):\n",
        "                if step == 0:\n",
        "                  print(\"1/\"+str(len(valid_dataloader)), end='')\n",
        "                else:\n",
        "                  print('\\b'*(len(str(step)) + len(str(len(valid_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(valid_dataloader)), end='')\n",
        "\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                b_input_ids, b_input_mask, b_labels, b_clust = batch\n",
        "\n",
        "                eval_output =  model(b_input_ids, b_input_mask, b_clust)\n",
        "                y_val += eval_output.detach().cpu().tolist()\n",
        "                loss = loss_function(eval_output, b_labels)\n",
        "\n",
        "                eval_loss += loss\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+14))\n",
        "\n",
        "        avg_eval_loss = eval_loss / len(valid_dataloader)\n",
        "        print(f\"Validation Loss: {avg_eval_loss}\")\n",
        "\n",
        "        # calculate extra statistics for the output\n",
        "        y_train = standardizer.destandardize(y_train)\n",
        "        y_val = standardizer.destandardize(y_val)\n",
        "\n",
        "        r2_train = r2_score(labels_train, y_train)\n",
        "        print(\"R² Train: \" + str(r2_train))\n",
        "        r2 = r2_score(labels_val, y_val)\n",
        "        print(\"R² Validation: \" + str(r2))\n",
        "\n",
        "        qerror_train = statistics.mean([max(labels_train[i], y_train[i])/min(labels_train[i], y_train[i] if y_train[i] > 0 else train_mean) for i in range(len(labels_train))])\n",
        "        print(\"Q-error Train: \" + str(qerror_train))\n",
        "        qerror = statistics.mean([max(labels_val[i], y_val[i])/min(labels_val[i], y_val[i] if y_val[i] > 0 else train_mean) for i in range(len(labels_val))])\n",
        "        print(\"Q-error Validation: \" + str(qerror))\n",
        "        print()\n",
        "\n",
        "        list_r2.append(r2)\n",
        "        list_r2_train.append(r2_train)\n",
        "        list_qerror.append(qerror)\n",
        "        list_qerror_train.append(qerror_train)\n",
        "        list_sample_size.append((epoch * 0.1 + 0.1))\n",
        "\n",
        "        # Early stopping based on validation loss\n",
        "        if avg_eval_loss < best_loss:\n",
        "            best_loss = avg_eval_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement == 5:\n",
        "                print(\"Validation loss has not improved in 5 epochs, stopping early.\")\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train model\n",
        "model = train(model, optimizer, scheduler, loss_function, epochs,\n",
        "              train_dataloader, valid_dataloader, device, clip_value=2)\n",
        "# Testing\n",
        "\n",
        "# define r2_score function\n",
        "def r2_score(outputs, labels):\n",
        "    labels_mean = statistics.mean(labels)\n",
        "    ss_tot = sum([(x-labels_mean)**2 for x in labels])\n",
        "    ss_res = sum([(x-y)**2 for x,y in zip(labels, outputs)])\n",
        "    r2 = (1 - (ss_res / (ss_tot if ss_tot != 0 else 1e-10)))\n",
        "    return r2\n",
        "\n",
        "# Measure performance on test set\n",
        "y_test = labels_test\n",
        "y_pred = predict(model, test_dataloader, device)\n",
        "y_pred = standardizer.destandardize(y_pred)\n",
        "\n",
        "# more imports for metrics to be used on test set\n",
        "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "import pandas as pd\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"MAE Test: \", mae)\n",
        "mdae = median_absolute_error(y_test, y_pred)\n",
        "print(\"MDAE Test: \", mdae)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"MSE Test: \", mse)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "print(\"MAPE Test: \", mape)\n",
        "mdape = ((pd.Series(y_test) - pd.Series(y_pred)) / pd.Series(y_test)).abs().median()\n",
        "print(\"MDAPE Test: \", mdape)\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "print(\"R² Test: \", r_squared)\n",
        "qerror = statistics.mean([max(y_test[i], y_pred[i] if y_pred[i] != 0 else train_mean)/min(y_test[i], y_pred[i] if y_pred[i] > 0 else train_mean) for i in range(len(y_test))])\n",
        "print(\"Q-error Test: \", qerror)"
      ],
      "metadata": {
        "id": "mX3URd0ezofP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Truncation, testing clusters individually"
      ],
      "metadata": {
        "id": "-uFeTONJ8Q8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastdtw"
      ],
      "metadata": {
        "id": "iWaY-jdYXSbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports\n",
        "import re\n",
        "import statistics\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from transformers import BertConfig, BertModel\n",
        "from fastdtw import fastdtw\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# function transforming numerical values in our dataset into tokens representing ranges\n",
        "def num_to_token(num):\n",
        "  if num < 1850:\n",
        "    return str(num)\n",
        "  if num < 2000:\n",
        "    return \"year1\"\n",
        "  elif num < 2010:\n",
        "    return \"year2\"\n",
        "  else:\n",
        "    return \"year3\"\n",
        "\n",
        "# function performing basic preprocessing\n",
        "def preprocess_query(query):\n",
        "  return re.sub(r\"(\\d+)\", lambda x: num_to_token(int(x.group(0))), query)\n",
        "\n",
        "# obtaining input queries and labels (cardinality or cost)\n",
        "input_queries = [preprocess_query(query) for query in <put desired query dataset here>]\n",
        "input_labels = <put desired target here> #(\"labels\" for JOB-light cardinality and \"costs\" for JOB-light costs)\n",
        "\n",
        "# limiting the number of queries to be used in the model\n",
        "input_queries = input_queries#[:number of queries to be used]\n",
        "input_labels = input_labels#[:number of queries to be used]\n",
        "\n",
        "# maximum query length before truncation, in number of tokens\n",
        "max_length = 256\n",
        "\n",
        "# tokenize dataset\n",
        "size = len(input_queries)\n",
        "\n",
        "# tokenize dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/bert_uncased_L-4_H-256_A-4', use_fast=True)\n",
        "encoded_corpus = tokenizer(text=input_queries,\n",
        "                            add_special_tokens=True,\n",
        "                            padding='max_length',\n",
        "                            truncation='longest_first',\n",
        "                            max_length=max_length,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt')\n",
        "input_ids = encoded_corpus['input_ids']\n",
        "attention_mask = encoded_corpus['attention_mask']\n",
        "\n",
        "# split train, validation and test sets\n",
        "input_ids_val = input_ids[int(size*0.6):int(size*0.8)]\n",
        "input_ids_train = input_ids[:int(size*0.6)]\n",
        "\n",
        "attention_mask_val = attention_mask[int(size*0.6):int(size*0.8)]\n",
        "attention_mask_train = attention_mask[:int(size*0.6)]\n",
        "\n",
        "labels_test = input_labels[int(size*0.8):]\n",
        "labels_val = input_labels[int(size*0.6):int(size*0.8)]\n",
        "labels_train = input_labels[:int(size*0.6)]\n",
        "\n",
        "# calculate the mean of the target in the train data\n",
        "train_mean = statistics.mean(labels_train)\n",
        "\n",
        "# Create segment embeddings\n",
        "segments = [[0] * max_length]\n",
        "# Initialize with zeroes\n",
        "for i in range(size-1):\n",
        "    segments += [[0] * max_length]\n",
        "# For every query\n",
        "for i, query in enumerate(input_queries):\n",
        "    broken_query = query.replace(\">=\", \" >= \")\n",
        "    broken_query = broken_query.replace(\"<=\", \" <= \")\n",
        "    broken_query = broken_query.replace(\"!=\", \" != \")\n",
        "    broken_query = broken_query.replace(\">\", \" > \")\n",
        "    broken_query = broken_query.replace(\"<\", \" < \")\n",
        "    broken_query = broken_query.replace(\"=\", \" = \")\n",
        "    broken_query = broken_query.replace(\",\", \" , \")\n",
        "    j = 1\n",
        "    state = 0\n",
        "    # Label each token based on its basic syntactic purpose\n",
        "    for word in broken_query.split(\" \"):\n",
        "        # First for keywords\n",
        "        if word == 'SELECT':\n",
        "          state = 0\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 0\n",
        "            j += 1\n",
        "        elif word == 'FROM':\n",
        "          state = 1\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 1\n",
        "            j += 1\n",
        "        elif word == 'WHERE':\n",
        "          state = 2\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 2\n",
        "            j += 1\n",
        "        elif word == 'COUNT(*)':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 3\n",
        "            j += 1\n",
        "        elif word == 'AND':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 4\n",
        "            j += 1\n",
        "        elif word == 'UNION':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 5\n",
        "            j += 1\n",
        "        elif word == 'IN':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 6\n",
        "            j += 1\n",
        "        elif word == '=':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 7\n",
        "            j += 1\n",
        "        elif word == '>=':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 8\n",
        "            j += 1\n",
        "        elif word == '<=':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 9\n",
        "            j += 1\n",
        "        elif word == '>':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 10\n",
        "            j += 1\n",
        "        elif word == '<':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 11\n",
        "            j += 1\n",
        "        elif word == ',':\n",
        "          for k in range(len(tokenizer.tokenize(word))):\n",
        "            segments[i][j] = 12\n",
        "            j += 1\n",
        "        # Then for other terms\n",
        "        else:\n",
        "          # Return columns and tables\n",
        "          if state < 2:\n",
        "            for k in range(len(tokenizer.tokenize(word))):\n",
        "              segments[i][j] = 13 + state\n",
        "              j += 1\n",
        "          else:\n",
        "            # Column in expression\n",
        "            if \".\" in word:\n",
        "              for k in range(len(tokenizer.tokenize(word))):\n",
        "                segments[i][j] = 15\n",
        "                j += 1\n",
        "            # Value\n",
        "            else:\n",
        "              for k in range(len(tokenizer.tokenize(word))):\n",
        "                segments[i][j] = 16\n",
        "                j += 1\n",
        "    segments[i][j] = segments[i][j-1]\n",
        "    segments[i] = segments[i][:j+1]\n",
        "\n",
        "# Calculate DTW distances in the train set and create a distance matrix\n",
        "n = len(segments)\n",
        "distance_matrix = np.zeros((n, n))\n",
        "for i in range(n):\n",
        "    for j in range(i, n):\n",
        "        distance, _ = fastdtw(segments[i], segments[j])\n",
        "        distance_matrix[i][j] = distance\n",
        "        distance_matrix[j][i] = distance\n",
        "\n",
        "# Perform K-Means clustering\n",
        "num_clusters = 70  # Adjust the number of clusters as needed\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(distance_matrix)\n",
        "\n",
        "# Extract points of each cluster\n",
        "cluster_points = [[] for _ in range(num_clusters)]\n",
        "for i in range(len(segments)):\n",
        "    cluster_index = kmeans.labels_[i]\n",
        "    cluster_points[cluster_index].append((input_ids[i], attention_mask[i],input_labels[i], segments[i]))\n",
        "\n",
        "# create test sets containing some queries from each cluster\n",
        "test_sets = [[] for _ in range(70)]\n",
        "for i in range(70):\n",
        "    test_sets[i].append(cluster_points[i].pop())\n",
        "    if len(cluster_points[i]) > 5:\n",
        "        test_sets[i] += cluster_points[i][4*len(cluster_points[i])//5:]\n",
        "        cluster_points[i] = cluster_points[i][:4*len(cluster_points[i])//5]\n",
        "import itertools\n",
        "test_set = list(itertools.chain.from_iterable(test_sets))\n",
        "\n",
        "input_ids_tests = [[] for _ in range(70)]\n",
        "for i in range(70):\n",
        "    input_ids_tests[i] = [j[0] for j in test_sets[i]]\n",
        "attention_mask_tests = [[] for _ in range(70)]\n",
        "for i in range(70):\n",
        "    attention_mask_tests[i] = [j[1] for j in test_sets[i]]\n",
        "labels_tests = [[] for _ in range(70)]\n",
        "for i in range(70):\n",
        "    labels_tests[i] = [j[2] for j in test_sets[i]]\n",
        "segments_tests = [[] for _ in range(70)]\n",
        "for i in range(70):\n",
        "    segments_tests[i] = [j[3] for j in test_sets[i]]\n",
        "input_ids_test = list(itertools.chain.from_iterable(input_ids_tests))\n",
        "attention_mask_test = list(itertools.chain.from_iterable(attention_mask_tests))\n",
        "labels_test = list(itertools.chain.from_iterable(labels_tests))\n",
        "segments_test = list(itertools.chain.from_iterable(segments_tests))\n",
        "predicted_from_clust_tests = labels_tests\n",
        "\n",
        "# create train and validation sets from the remaining data\n",
        "all_remaining = list(itertools.chain.from_iterable(cluster_points))\n",
        "train_all = all_remaining[:int(size*0.75)]\n",
        "input_ids_train = [j[0] for j in train_all]\n",
        "attention_mask_train = [j[1] for j in train_all]\n",
        "labels_train = [j[2] for j in train_all]\n",
        "segments_train = [j[3] for j in train_all]\n",
        "val_all = all_remaining[int(size*0.75):]\n",
        "input_ids_val = [j[0] for j in train_all]\n",
        "attention_mask_val = [j[1] for j in train_all]\n",
        "labels_val = [j[2] for j in train_all]\n",
        "segments_val = [j[3] for j in train_all]\n",
        "\n",
        "# Average the points of each cluster\n",
        "centroid_labels = []\n",
        "for points in cluster_points:\n",
        "    cur_sum = 0\n",
        "    for point in points:\n",
        "        cur_sum += 5\n",
        "    average = cur_sum/5\n",
        "    centroid_labels.append(average)\n",
        "\n",
        "# Get the cluster centroids\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "# Match the train set to a clusters\n",
        "predicted_from_clust_train = []\n",
        "for i in range(len(segments_train)):\n",
        "    matching_centr_label = centroid_labels[0]\n",
        "    predicted_from_clust_train.append(matching_centr_label)\n",
        "\n",
        "# Match the val set to a clusters\n",
        "predicted_from_clust_val = []\n",
        "for i in range(len(segments_val)):\n",
        "    matching_centr_label = centroid_labels[0]\n",
        "    predicted_from_clust_val.append(matching_centr_label)\n",
        "\n",
        "# Match the test set to a cluster\n",
        "predicted_from_clust_test = []\n",
        "for i in range(len(segments_test)):\n",
        "    matching_centr_label = centroid_labels[0]\n",
        "    predicted_from_clust_test.append(matching_centr_label)\n",
        "\n",
        "# standardize all data, based on the statistics of the train set\n",
        "class Standardizer:\n",
        "    def __init__(self):\n",
        "        self.means = 0\n",
        "        self.stds = 0\n",
        "\n",
        "    def standardize(self, train, validation, test, clust_train, clust_val, clust_test):\n",
        "        train_stand = torch.tensor(train)\n",
        "        validation_stand = torch.tensor(validation)\n",
        "        test_stand = torch.tensor(test)\n",
        "        clust_train_stand = torch.tensor(clust_train)\n",
        "        clust_val_stand = torch.tensor(clust_val)\n",
        "        clust_test_stand = torch.tensor(clust_test)\n",
        "\n",
        "        self.stds = train_stand.std(dim=0, keepdim=True)\n",
        "        train_stand = train_stand / self.stds\n",
        "        validation_stand = validation_stand / self.stds\n",
        "        test_stand = test_stand / self.stds\n",
        "        clust_train_stand = clust_train_stand / self.stds\n",
        "        clust_val_stand = clust_val_stand / self.stds\n",
        "        clust_test_stand = clust_test_stand / self.stds\n",
        "\n",
        "        train_stand = train_stand.detach().cpu().tolist()\n",
        "        validation_stand = validation_stand.detach().cpu().tolist()\n",
        "        test_stand = test_stand.detach().cpu().tolist()\n",
        "        clust_train_stand = clust_train_stand.detach().cpu().tolist()\n",
        "        clust_val_stand = clust_val_stand.detach().cpu().tolist()\n",
        "        clust_test_stand = clust_test_stand.detach().cpu().tolist()\n",
        "        return train_stand, validation_stand, test_stand, clust_train_stand, clust_val_stand, clust_test_stand\n",
        "\n",
        "    def destandardize(self, data):\n",
        "        data_destand = torch.tensor(data)\n",
        "        data_destand = data_destand*self.stds\n",
        "        data_destand = data_destand.detach().cpu().tolist()\n",
        "        return data_destand\n",
        "\n",
        "standardizer = Standardizer()\n",
        "labels_train, labels_val, labels_test, predicted_from_clust_train, predicted_from_clust_val, predicted_from_clust_test = standardizer.standardize(labels_train, labels_val, labels_test, predicted_from_clust_train, predicted_from_clust_val, predicted_from_clust_test)\n",
        "\n",
        "# make dataloaders\n",
        "batch_size = 64\n",
        "\n",
        "def make_dataloader(inputs, masks, labels, predicted_from_clust, batch_size):\n",
        "    predicted_from_clust = [0]*len(labels)\n",
        "    inputs_tens = torch.stack(inputs)\n",
        "    masks_tens = torch.stack(masks)\n",
        "    labels_tens = torch.tensor(labels)\n",
        "    predicted_from_clust_tens = torch.tensor(predicted_from_clust)\n",
        "    dataset = TensorDataset(inputs_tens, masks_tens, labels_tens, predicted_from_clust_tens)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    return dataloader\n",
        "\n",
        "train_dataloader = make_dataloader(input_ids_train, attention_mask_train, labels_train, predicted_from_clust_train, batch_size)\n",
        "valid_dataloader = make_dataloader(input_ids_val, attention_mask_val, labels_val, predicted_from_clust_val, batch_size)\n",
        "test_dataloader = make_dataloader(input_ids_test, attention_mask_test, labels_test, predicted_from_clust_test, batch_size)\n",
        "test_dataloaders = []\n",
        "for i in range(70):\n",
        "  test_dataloaders.append(make_dataloader(input_ids_tests[i],attention_mask_tests[i],labels_tests[i],predicted_from_clust_tests[i], batch_size))\n",
        "\n",
        "# destandardize labels after passing them to the dataloaders,\n",
        "# we want them standardized within the model and unstandardized outside\n",
        "labels_train = standardizer.destandardize(labels_train)\n",
        "labels_val = standardizer.destandardize(labels_val)\n",
        "labels_test = standardizer.destandardize(labels_test)\n",
        "predicted_from_clust_train = standardizer.destandardize(predicted_from_clust_train)\n",
        "predicted_from_clust_val = standardizer.destandardize(predicted_from_clust_val)\n",
        "predicted_from_clust_test = standardizer.destandardize(predicted_from_clust_test)\n",
        "\n",
        "# load BERT model and define our head\n",
        "config = BertConfig.from_pretrained('google/bert_uncased_L-4_H-256_A-4', num_labels=2, hidden_dropout_prob=0.3,\n",
        "                                    attention_probs_dropout_prob=0.3, output_attentions = False, output_hidden_states = True)\n",
        "\n",
        "\n",
        "class BertRegressor(nn.Module):\n",
        "\n",
        "    def __init__(self, drop_rate=0.3, config=config):\n",
        "\n",
        "        super(BertRegressor, self).__init__()\n",
        "        D_in, D_out = 256, 1\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('google/bert_uncased_L-4_H-256_A-4', config=config)\n",
        "        self.drop = nn.Dropout(drop_rate)\n",
        "        self.linear = nn.Linear(D_in, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(2, D_out)\n",
        "        self.relu2 = nn.ReLU()\n",
        "    def forward(self, input_ids, attention_masks, predicted_from_clust):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_masks)\n",
        "        hidden_states = outputs[2]\n",
        "        token_vecs = hidden_states[-2][:]\n",
        "        token_vecs = token_vecs.permute(1,0,2)\n",
        "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "        outputs = self.drop(sentence_embedding)\n",
        "        outputs = self.linear(outputs)\n",
        "        outputs = self.relu(outputs)\n",
        "        outputs = torch.squeeze(outputs, 1)\n",
        "        outputs = torch.stack((outputs, predicted_from_clust),1)\n",
        "        outputs = self.linear2(outputs)\n",
        "        outputs = self.relu2(outputs)\n",
        "        outputs = torch.squeeze(outputs, 1)\n",
        "        return outputs\n",
        "\n",
        "model = BertRegressor()\n",
        "\n",
        "# connect to GPU, if available\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# define optimizer, with scheduler for adapting learning rate\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 50 # define maximum number of epochs\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=steps)\n",
        "\n",
        "# define loss function\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# imports for statistics\n",
        "from sklearn.metrics import r2_score\n",
        "import statistics\n",
        "\n",
        "# define statistics\n",
        "list_r2=[]\n",
        "list_r2_train=[]\n",
        "list_qerror=[]\n",
        "list_qerror_train=[]\n",
        "list_sample_size=[]\n",
        "\n",
        "# function to predict using model\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    output = []\n",
        "    for batch in dataloader:\n",
        "        batch_inputs, batch_masks, batch_labels, batch_clust = tuple(b.to(device) for b in batch)\n",
        "        with torch.no_grad():\n",
        "            output += model(batch_inputs, batch_masks, batch_clust).view(1,-1).tolist()[0]\n",
        "    return output\n",
        "\n",
        "# function to train model\n",
        "def train(model, optimizer, scheduler, loss_function, epochs,\n",
        "          train_dataloader, valid_dataloader, device, clip_value=2):\n",
        "    best_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # print progress\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(\"-----\")\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        y_train = []\n",
        "        print(\"Training:\")\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            if step == 0:\n",
        "              print(\"1/\"+str(len(train_dataloader)), end='')\n",
        "            else:\n",
        "              print('\\b'*(len(str(step)) + len(str(len(train_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(train_dataloader)), end='')\n",
        "\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels, b_clust = batch\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            predictions = model(b_input_ids, b_input_mask, b_clust)\n",
        "            y_train += predictions.detach().cpu().tolist()\n",
        "            loss = loss_function(predictions, b_labels)\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "            # Update model's weights based on the gradients calculated during backpropagation\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+12))\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_dataloader)\n",
        "        print(f\"Training Loss: {avg_train_loss}\")\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        model.eval()\n",
        "        eval_loss = 0.0\n",
        "\n",
        "        y_val = []\n",
        "        with torch.no_grad():\n",
        "            print(\"Validating:\")\n",
        "            for step, batch in enumerate(valid_dataloader):\n",
        "                if step == 0:\n",
        "                  print(\"1/\"+str(len(valid_dataloader)), end='')\n",
        "                else:\n",
        "                  print('\\b'*(len(str(step)) + len(str(len(valid_dataloader))) + 1) + str(step + 1) + \"/\" + str(len(valid_dataloader)), end='')\n",
        "\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                b_input_ids, b_input_mask, b_labels, b_clust = batch\n",
        "\n",
        "                eval_output =  model(b_input_ids, b_input_mask, b_clust)\n",
        "                y_val += eval_output.detach().cpu().tolist()\n",
        "                loss = loss_function(eval_output, b_labels)\n",
        "\n",
        "                eval_loss += loss\n",
        "\n",
        "        print('\\b'*(2*len(str(len(train_dataloader)))+14))\n",
        "\n",
        "        avg_eval_loss = eval_loss / len(valid_dataloader)\n",
        "        print(f\"Validation Loss: {avg_eval_loss}\")\n",
        "\n",
        "        # calculate extra statistics for the output\n",
        "        y_train = standardizer.destandardize(y_train)\n",
        "        y_val = standardizer.destandardize(y_val)\n",
        "\n",
        "        r2_train = r2_score(labels_train, y_train)\n",
        "        print(\"R² Train: \" + str(r2_train))\n",
        "        r2 = r2_score(labels_val, y_val)\n",
        "        print(\"R² Validation: \" + str(r2))\n",
        "\n",
        "        qerror_train = statistics.mean([max(labels_train[i], y_train[i])/min(labels_train[i], y_train[i] if y_train[i] > 0 else train_mean) for i in range(len(labels_train))])\n",
        "        print(\"Q-error Train: \" + str(qerror_train))\n",
        "        qerror = statistics.mean([max(labels_val[i], y_val[i])/min(labels_val[i], y_val[i] if y_val[i] > 0 else train_mean) for i in range(len(labels_val))])\n",
        "        print(\"Q-error Validation: \" + str(qerror))\n",
        "        print()\n",
        "\n",
        "        list_r2.append(r2)\n",
        "        list_r2_train.append(r2_train)\n",
        "        list_qerror.append(qerror)\n",
        "        list_qerror_train.append(qerror_train)\n",
        "        list_sample_size.append((epoch * 0.1 + 0.1))\n",
        "\n",
        "        # Early stopping based on validation loss\n",
        "        if avg_eval_loss < best_loss:\n",
        "            best_loss = avg_eval_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement == 5:\n",
        "                print(\"Validation loss has not improved in 5 epochs, stopping early.\")\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train model\n",
        "model = train(model, optimizer, scheduler, loss_function, epochs,\n",
        "              train_dataloader, valid_dataloader, device, clip_value=2)\n",
        "# Testing\n",
        "\n",
        "# define r2_score function\n",
        "def r2_score(outputs, labels):\n",
        "    labels_mean = statistics.mean(labels)\n",
        "    ss_tot = sum([(x-labels_mean)**2 for x in labels])\n",
        "    ss_res = sum([(x-y)**2 for x,y in zip(labels, outputs)])\n",
        "    r2 = (1 - (ss_res / (ss_tot if ss_tot != 0 else 1e-10)))\n",
        "    return r2\n",
        "\n",
        "# Measure performance on test set\n",
        "y_test = labels_test\n",
        "y_pred = predict(model, test_dataloader, device)\n",
        "y_pred = standardizer.destandardize(y_pred)\n",
        "\n",
        "# more imports for metrics to be used on test set\n",
        "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "import pandas as pd\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"MAE Test: \", mae)\n",
        "mdae = median_absolute_error(y_test, y_pred)\n",
        "print(\"MDAE Test: \", mdae)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"MSE Test: \", mse)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "print(\"MAPE Test: \", mape)\n",
        "mdape = ((pd.Series(y_test) - pd.Series(y_pred)) / pd.Series(y_test)).abs().median()\n",
        "print(\"MDAPE Test: \", mdape)\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "print(\"R² Test: \", r_squared)\n",
        "qerror = statistics.mean([max(y_test[i], y_pred[i] if y_pred[i] != 0 else train_mean)/min(y_test[i], y_pred[i] if y_pred[i] > 0 else train_mean) for i in range(len(y_test))])\n",
        "print(\"Q-error Test: \", qerror)\n",
        "\n",
        "qerrors = []\n",
        "# Measure performance on each of the 70 test sets\n",
        "for i in range(70):\n",
        "  print(i)\n",
        "  y_test = labels_tests[i]\n",
        "  y_pred = predict(model, test_dataloaders[i], device)\n",
        "  y_pred = standardizer.destandardize(y_pred)\n",
        "\n",
        "  mae = mean_absolute_error(y_test, y_pred)\n",
        "  print(\"MAE Test: \", mae)\n",
        "  mdae = median_absolute_error(y_test, y_pred)\n",
        "  print(\"MDAE Test: \", mdae)\n",
        "  mse = mean_squared_error(y_test, y_pred)\n",
        "  print(\"MSE Test: \", mse)\n",
        "  mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "  print(\"MAPE Test: \", mape)\n",
        "  mdape = ((pd.Series(y_test) - pd.Series(y_pred)) / pd.Series(y_test)).abs().median()\n",
        "  print(\"MDAPE Test: \", mdape)\n",
        "  r_squared = r2_score(y_test, y_pred)\n",
        "  print(\"R² Test: \", r_squared)\n",
        "  qerror = statistics.mean([max(y_test[i], y_pred[i] if y_pred[i] != 0 else train_mean)/min(y_test[i], y_pred[i] if y_pred[i] > 0 else train_mean) for i in range(len(y_test))])\n",
        "  print(\"Q-error Test: \", qerror)\n",
        "  qerrors.append(qerror)\n",
        "\n",
        "  for j in range(len(y_test)):\n",
        "    mae = mean_absolute_error([y_test[j]], [y_pred[j]])\n",
        "    #print(\"MAE Test: \", mae)\n",
        "    mdae = median_absolute_error([y_test[j]], [y_pred[j]])\n",
        "    #print(\"MDAE Test: \", mdae)\n",
        "    mse = mean_squared_error([y_test[j]], [y_pred[j]])\n",
        "    #print(\"MSE Test: \", mse)\n",
        "    mape = mean_absolute_percentage_error([y_test[j]], [y_pred[j]])\n",
        "    #print(\"MAPE Test: \", mape)\n",
        "    mdape = ((pd.Series([y_test[j]]) - pd.Series([y_pred[j]])) / pd.Series([y_test[j]])).abs().median()\n",
        "    #print(\"MDAPE Test: \", mdape)\n",
        "    r_squared = r2_score([y_test[j]], [y_pred[j]])\n",
        "    print(\"R² Test: \", r_squared)\n",
        "    qerror = max(y_test[j], y_pred[j] if y_pred[j] != 0 else train_mean)/min(y_test[j], y_pred[j] if y_pred[j] > 0 else train_mean)\n",
        "    print(\"Q-error Test: \", qerror)\n",
        "\n",
        "# Your list of numbers\n",
        "categories = [\" \"*i for i in range(70)]\n",
        "\n",
        "# Create a bar chart\n",
        "plt.bar(categories, qerrors)\n",
        "\n",
        "# Add labels and a title\n",
        "plt.xlabel('Sets')\n",
        "plt.ylabel('q-error')\n",
        "plt.title('q-error metric for various test subsets')\n",
        "\n",
        "# Display the chart\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Crwk8ENj9lK7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hiVtkawhgFvz",
        "JHmsY25WrZ3t",
        "3UGhrW9DreP6",
        "fg4k7Y2xsB__",
        "8t-BJwr1y2xa",
        "nExHKj1ay9-W",
        "uVIBUPCDy_0U",
        "P-PSHz7TzC23",
        "QJ-STRFny6_d",
        "-uFeTONJ8Q8-"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}